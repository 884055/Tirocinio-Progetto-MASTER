{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASTER - Notebook 4\n",
    "### Matteo Grazioso 884055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import folium\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import myfunctions as mf # Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disply all columns and all rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_dataset_to_period(df, start_date, end_date):\n",
    "    '''\n",
    "    Restrict the dataset to only the specified period given by the user\n",
    "        :param df: the dataset to be restricted\n",
    "        :param start_date: the start date of the period\n",
    "        :param end_date: the end date of the period\n",
    "        :return: the restricted dataset        \n",
    "    ''' \n",
    "\n",
    "    # Filter the dataset to only the specified period\n",
    "    df = df[(df['DATA'] >= start_date) & (df['DATA'] <= end_date)]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If does not exist a file named data/processed/dataset_cleaned_temp02-04_2023-02-21_2-esportazioneCompleta.txt create it\n",
    "# Carnival period: 2023-02-04 - 2023-02-21\n",
    "if not os.path.exists('data/processed/dataset_cleaned_temp02-04_2023-02-21_2-esportazioneCompleta.txt'):\n",
    "    path = 'data/raw/2-esportazioneCompleta.txt'\n",
    "    df = pd.read_csv(path, header=0, sep='\\t')\n",
    "\n",
    "    # Save the name of the file in a variable for future use extracting the name of the file from the path\n",
    "    file_name = path.split('_')[-1].split('/')[2]\n",
    "\n",
    "    # Convert the column 'DATA' to datetime format\n",
    "    df.insert(0, 'DATA', pd.to_datetime(df['DATA_VALIDAZIONE'].str.split(' ').str[0], format='%d/%m/%Y'))\n",
    "    df.insert(1, 'ORA', pd.to_datetime(df['DATA_VALIDAZIONE'].str.split(' ').str[1], format='%H:%M').dt.time)\n",
    "    df['DATA'] = pd.to_datetime(df['DATA'], format='%Y-%m-%d')\n",
    "\n",
    "    # Display the first 5 rows of the dataframe\n",
    "    print (df.head())\n",
    "\n",
    "    # Print information about the dataset\n",
    "    print('df.shape: ', df.shape)\n",
    "\n",
    "    # Define the interval of dates to restrict the dataset\n",
    "    start_date = '2023-02-04'\n",
    "    end_date = '2023-02-21'\n",
    "    df = restrict_dataset_to_period(df, start_date, end_date)\n",
    "\n",
    "    # Remove DATA and ORA columns\n",
    "    df = df.drop(['DATA', 'ORA'], axis=1)\n",
    "\n",
    "    # Export the data to a txt file\n",
    "    name_file = 'restricted_' + str(start_date) + '_' + str(end_date) + '_' + file_name\n",
    "    df.to_csv('data/raw/' + name_file, sep='\\t', index=False)\n",
    "\n",
    "    print('Data exported to ' + name_file)\n",
    "else :\n",
    "    print('Dataset restricted to the period: 2023-02-04 - 2023-02-21 already exists')\n",
    "    print('Path: data/processed/dataset_cleaned_temp02-04_2023-02-21_2-esportazioneCompleta.txt')\n",
    "    # Open the file\n",
    "    df = pd.read_csv('data/processed/dataset_cleaned_temp02-04_2023-02-21_2-esportazioneCompleta.txt', sep='\\t')\n",
    "    # Print information about the dataset\n",
    "    print('df.shape: ', df.shape)\n",
    "    print('df.head(2)')\n",
    "    print(df.head(2))\n",
    "\n",
    "    # Print the interval of dates (DATA column) of the dataset\n",
    "    # Min date\n",
    "    print('Min date: ', df['DATA'].min())\n",
    "    # Max date\n",
    "    print('Max date: ', df['DATA'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If does not exist a file named data/processed/dataset_cleaned_temp02-04_2023-02-21_2-esportazioneCompleta.txt create it\n",
    "# Period after Carnival: 2023-02-22 - 2023-03-12\n",
    "if not os.path.exists('data/processed/dataset_cleaned_temp02-22_2023-03-12_2-esportazioneCompleta.txt'):\n",
    "    path = 'data/raw/2-esportazioneCompleta.txt'\n",
    "    df = pd.read_csv(path, header=0, sep='\\t')\n",
    "\n",
    "    # Save the name of the file in a variable for future use extracting the name of the file from the path\n",
    "    file_name = path.split('_')[-1].split('/')[2]\n",
    "\n",
    "    # Convert the column 'DATA' to datetime format\n",
    "    df.insert(0, 'DATA', pd.to_datetime(df['DATA_VALIDAZIONE'].str.split(' ').str[0], format='%d/%m/%Y'))\n",
    "    df.insert(1, 'ORA', pd.to_datetime(df['DATA_VALIDAZIONE'].str.split(' ').str[1], format='%H:%M').dt.time)\n",
    "    df['DATA'] = pd.to_datetime(df['DATA'], format='%Y-%m-%d')\n",
    "\n",
    "    # Display the first 5 rows of the dataframe\n",
    "    print (df.head())\n",
    "\n",
    "    # Print information about the dataset\n",
    "    print('df.shape: ', df.shape)\n",
    "\n",
    "    # Define the interval of dates to restrict the dataset\n",
    "    start_date = '2023-02-22'\n",
    "    end_date = '2023-03-12'\n",
    "    df = restrict_dataset_to_period(df, start_date, end_date)\n",
    "\n",
    "    # Remove DATA and ORA columns\n",
    "    df = df.drop(['DATA', 'ORA'], axis=1)\n",
    "\n",
    "    # Export the data to a txt file\n",
    "    name_file = 'restricted_' + str(start_date) + '_' + str(end_date) + '_' + file_name\n",
    "    df.to_csv('data/raw/' + name_file, sep='\\t', index=False)\n",
    "\n",
    "    print('Data exported to ' + name_file)\n",
    "else :\n",
    "    print('Dataset restricted to the period: 2023-02-22 - 2023-03-12 already exists')\n",
    "    print('Path: data/processed/dataset_cleaned_temp02-22_2023-03-12_2-esportazioneCompleta.txt')\n",
    "    # Open the file\n",
    "    df = pd.read_csv('data/processed/dataset_cleaned_temp02-22_2023-03-12_2-esportazioneCompleta.txt', header=0, sep='\\t')\n",
    "    # Print information about the dataset\n",
    "    print('df.shape: ', df.shape)\n",
    "    print('df.head(2)')\n",
    "    print(df.head(2))\n",
    "\n",
    "    # Print the interval of dates (DATA column) of the dataset\n",
    "    # Min date\n",
    "    print('Min date: ', df['DATA'].min())\n",
    "    # Max date\n",
    "    print('Max date: ', df['DATA'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If does not exist a file named data/processed/dataset_cleaned_temp01-17_2023-02-03_2-esportazioneCompleta.txt create it\n",
    "# Period before Carnival: 2023-01-17 - 2023-02-03\n",
    "if not os.path.exists('data/processed/dataset_cleaned_temp01-17_2023-02-03_2-esportazioneCompleta.txt'):\n",
    "    path = 'data/raw/2-esportazioneCompleta.txt'\n",
    "    df = pd.read_csv(path, header=0, sep='\\t')\n",
    "\n",
    "    # Save the name of the file in a variable for future use extracting the name of the file from the path\n",
    "    file_name = path.split('_')[-1].split('/')[2]\n",
    "\n",
    "    # Convert the column 'DATA' to datetime format\n",
    "    df.insert(0, 'DATA', pd.to_datetime(df['DATA_VALIDAZIONE'].str.split(' ').str[0], format='%d/%m/%Y'))\n",
    "    df.insert(1, 'ORA', pd.to_datetime(df['DATA_VALIDAZIONE'].str.split(' ').str[1], format='%H:%M').dt.time)\n",
    "    df['DATA'] = pd.to_datetime(df['DATA'], format='%Y-%m-%d')\n",
    "\n",
    "    # Display the first 5 rows of the dataframe\n",
    "    print (df.head())\n",
    "\n",
    "    # Print information about the dataset\n",
    "    print('df.shape: ', df.shape)\n",
    "\n",
    "    # Define the interval of dates to restrict the dataset\n",
    "    start_date = '2023-01-17'\n",
    "    end_date = '2023-02-03'\n",
    "    df = restrict_dataset_to_period(df, start_date, end_date)\n",
    "\n",
    "    # Remove DATA and ORA columns\n",
    "    df = df.drop(['DATA', 'ORA'], axis=1)\n",
    "\n",
    "    # Export the data to a txt file\n",
    "    name_file = 'restricted_' + str(start_date) + '_' + str(end_date) + '_' + file_name\n",
    "    df.to_csv('data/raw/' + name_file, sep='\\t', index=False)\n",
    "\n",
    "    print('Data exported to ' + name_file)\n",
    "else :\n",
    "    print('Dataset restricted to the period: 2023-01-17 - 2023-02-03 already exists')\n",
    "    print('Path: data/processed/dataset_cleaned_temp01-17_2023-02-03_2-esportazioneCompleta.txt')\n",
    "    # Open the file\n",
    "    df = pd.read_csv('data/processed/dataset_cleaned_temp01-17_2023-02-03_2-esportazioneCompleta.txt', header=0, sep='\\t')\n",
    "    # Print information about the dataset\n",
    "    print('df.shape: ', df.shape)\n",
    "    print('df.head(2)')\n",
    "    print(df.head(2))\n",
    "\n",
    "    # Print the interval of dates (DATA column) of the dataset\n",
    "    # Min date\n",
    "    print('Min date: ', df['DATA'].min())\n",
    "    # Max date\n",
    "    print('Max date: ', df['DATA'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dataset must be cleaned before it can be used. The cleaning process is done in the following steps:\n",
    "# 1. Execute the notebook 1_b_only_temp_cleaning.ipynb to clean the dataset deleting useless stamps once the algorithm has determined the minimum temporal gap between two consecutive stamps.\n",
    "\n",
    "# The result of the cleaning process is a new dataset that must be used to obtain the dataset with geographical coordinates. This process is done in the following steps:\n",
    "# 1. Execute the Notebook 3 AUX.ipynb to obtain the dataset with geographical coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset that has also the geo coordinates\n",
    "# Find all txt files in the data folder\n",
    "csv_file = mf.find_csv_files('data/processed/')\n",
    "\n",
    "print(\"Select a dataset with geo coordinates from the list:\")\n",
    "\n",
    "# Choose a dataset from the list of txt files\n",
    "selected_dataset = mf.choose_dataset(csv_file)\n",
    "\n",
    "if selected_dataset:\n",
    "    print(f\"You selected the dataset {selected_dataset}\")\n",
    "else:\n",
    "    print(\"No dataset selected.\")\n",
    "\n",
    "path  = selected_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, header=0, sep=',')\n",
    "\n",
    "# Save the name of the file in a variable for future use extracting the name of the file from the path\n",
    "file_name = path.split('_')[1]\n",
    "# If file_name has a slash, split it and take the first element\n",
    "if '/' in file_name:\n",
    "    file_name = file_name.split('/')[0]\n",
    "subfolder = file_name\n",
    "print(f\"File name: {file_name}\")\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "df.head()\n",
    "\n",
    "# Convert the column 'DATA' to datetime format\n",
    "df['DATA'] = pd.to_datetime(df['DATA'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the interval of dates for which we have data\n",
    "print('Date range: {} to {}'.format(df['DATA'].min(), df['DATA'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each stop, store the number of use for each ticket code\n",
    "# Each stop is a point identified by the coordinates (latitude, longitude)\n",
    "\n",
    "with open('data/dictionaries/dict_ticket_codes.json') as f:\n",
    "        ticket_codes = json.load(f)\n",
    "\n",
    "print('The ticket codes are: ', ticket_codes)\n",
    "\n",
    "# Change 5-STUD, 6-STUD to STUD in the dataframe\n",
    "df['TICKET_CODE'] = df['TICKET_CODE'].replace(['5-STUD', '6-STUD'], 'STUD')\n",
    "# Change 5-WKRS, 6-WKRS to WKRS in the dataframe\n",
    "df['TICKET_CODE'] = df['TICKET_CODE'].replace(['5-WKRS', '6-WKRS'], 'WKRS')\n",
    "# Change 5-RET, 6-RET to RET in the dataframe\n",
    "df['TICKET_CODE'] = df['TICKET_CODE'].replace(['5-RET', '6-RET'], 'RET')\n",
    "\n",
    "# Print the unique ticket codes\n",
    "# Print information about the changes made\n",
    "print('The ticket codes 5-STUD and 6-STUD have been changed to STUD')\n",
    "print('The ticket codes 5-WKRS and 6-WKRS have been changed to WKRS')\n",
    "print('The ticket codes 5-RET and 6-RET have been changed to RET')\n",
    "\n",
    "# Convert all the ticket codes to string\n",
    "df['TICKET_CODE'] = df['TICKET_CODE'].astype(str)\n",
    "\n",
    "ticket_codes = df['TICKET_CODE'].unique()\n",
    "# Sort the ticket codes\n",
    "ticket_codes.sort()\n",
    "\n",
    "print('The considered ticket codes are: ', ticket_codes)\n",
    "\n",
    "# For each stop, store the number of visits for each ticket code\n",
    "# Iterate over the stops dataframe and for each stop, store the number of visits for each ticket code\n",
    "# Notice that a stop is a pair of coordinates (latitude, longitude)\n",
    "\n",
    "# The columns of the dataframe are:\n",
    "# ['DATA', 'ORA', 'DATA_VALIDAZIONE', 'SERIALE', 'FERMATA', 'DESCRIZIONE',\n",
    "#        'TITOLO', 'TICKET_CODE', 'DESCRIZIONE_TITOLO', 'LATITUDE', 'LONGITUDE']\n",
    "\n",
    "# Create a dataframe of stops\n",
    "df_stop = df[['LATITUDE', 'LONGITUDE', 'TICKET_CODE']]\n",
    "\n",
    "df_stop.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_count = df_stop.groupby(['LATITUDE', 'LONGITUDE', 'TICKET_CODE']).size().reset_index(name='COUNT')\n",
    "\n",
    "df_stop_count.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the column COUNT\n",
    "df_stop_count['COUNT'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for the ticket codes\n",
    "df_stop_count = df_stop_count.pivot_table(index=['LATITUDE', 'LONGITUDE'], columns='TICKET_CODE', values='COUNT', fill_value=0)\n",
    "df_stop_count.reset_index(inplace=True)\n",
    "\n",
    "# For each stop (LATITUDE, LONGITUDE), change the counter of each ticket code as a percentage of the total number of tickets\n",
    "for row in range(len(df_stop_count)):\n",
    "    total = df_stop_count.iloc[row, 2:].sum()\n",
    "    for col in range(2, len(df_stop_count.columns)):\n",
    "        df_stop_count.iloc[row, col] = df_stop_count.iloc[row, col] / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def haversine_distance(coord1, coord2):\n",
    "    \"\"\"\n",
    "        Calculate the distance between two points on Earth using the haversine formula.\n",
    "        The haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes.\n",
    "        The haversin formula is specified as:\n",
    "            a = sin²(Δlat/2) + cos(lat1).cos(lat2).sin²(Δlong/2)\n",
    "            c = 2.atan2(√a, √(1−a))\n",
    "            d = R.c\n",
    "        where:\n",
    "            lat1, long1 = Latitude and Longitude of point 1 (in decimal degrees)\n",
    "            lat2, long2 = Latitude and Longitude of point 2 (in decimal degrees)\n",
    "            R = Radius of the Earth in kilometers\n",
    "            Δlat = lat2− lat1\n",
    "            Δlong = long2− long1\n",
    "\n",
    "        :param coord1: Tuple of (latitude, longitude) for point 1\n",
    "        :param coord2: Tuple of (latitude, longitude) for point 2\n",
    "        :return: Distance between the two coordinates in kilometers\n",
    "    \"\"\"\n",
    "    lon1, lat1 = coord1\n",
    "    lon2, lat2 = coord2\n",
    "    \n",
    "    R = 6371  # Radius of the Earth in kilometers\n",
    "    \n",
    "    # Convert decimal degrees to radians\n",
    "    dlat = np.radians(lat2 - lat1)\n",
    "    dlon = np.radians(lon2 - lon1)\n",
    "    \n",
    "    # Apply haversine formula\n",
    "    # a = sin²(Δlat/2) + cos(lat1).cos(lat2).sin²(Δlong/2)\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon / 2) ** 2\n",
    "\n",
    "    # c = 2.atan2(√a, √(1−a))\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    # d = R.c\n",
    "    distance = R * c\n",
    "\n",
    "    # print('Distance between stops: ', distance)\n",
    "    return distance\n",
    "\n",
    "def cosine_similarity_distance(counts1, counts2):\n",
    "    # Calculate cosine similarity between two count vectors\n",
    "    \"\"\"\n",
    "        Calculate the cosine similarity between two count vectors.\n",
    "        The cosine similarity is a measure of similarity between two non-zero vectors of an inner product space \n",
    "        that measures the cosine of the angle between them.\n",
    "        The cosine of 0° is 1, and it is less than 1 for any other angle.\n",
    "        It is thus a judgement of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1,\n",
    "        two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude.\n",
    "        The cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1].\n",
    "        The cosine similarity is mathematically defined as:\n",
    "            cos(θ) = A.B / ||A||.||B||\n",
    "        where:\n",
    "            A.B = Dot product of A and B\n",
    "            ||A|| = Euclidean norm of A\n",
    "            ||B|| = Euclidean norm of B\n",
    "        :param counts1: List of counts for each ticket code for stop 1\n",
    "        :param counts2: List of counts for each ticket code for stop 2\n",
    "        :return: Cosine similarity between the two count vectors\n",
    "\n",
    "        # This function calls the cosine_similarity function from sklearn.metrics.pairwise to calculate the cosine similarity between two count vectors.\n",
    "    \"\"\"\n",
    "    similarity_matrix = cosine_similarity([counts1], [counts2])\n",
    "    similarity = 1 - similarity_matrix[0][0]\n",
    "    \n",
    "    # print('Similarity between stops: ', similarity)\n",
    "    return similarity\n",
    "\n",
    "def custom_distance(stop1, stop2, coord_weight, similarity_weight):\n",
    "    \"\"\"\n",
    "        Calculate the custom distance between two stops.\n",
    "        The custom distance is a weighted combination of the haversine distance between the two stops and the cosine similarity between the two stops.\n",
    "        The custom distance is mathematically defined as:\n",
    "            custom_distance = coord_weight * haversine_distance + similarity_weight * cosine_similarity_distance\n",
    "        where:\n",
    "            coord_weight = Weight for haversine distance\n",
    "            similarity_weight = Weight for cosine similarity distance\n",
    "        :param stop1: Tuple of (latitude, longitude, counts for each ticket code) for stop 1\n",
    "        :param stop2: Tuple of (latitude, longitude, counts for each ticket code) for stop 2\n",
    "        :param coord_weight: Weight for haversine distance\n",
    "        :param similarity_weight: Weight for cosine similarity distance\n",
    "        :return: Custom distance between the two stops\n",
    "    \"\"\"\n",
    "    # Calculate distances\n",
    "    # Calculate haversine distance between two stops\n",
    "    coord_distance = haversine_distance((stop1[0], stop1[1]), (stop2[0], stop2[1]))\n",
    "    # Calculate cosine similarity between two stops\n",
    "    count_similarity = cosine_similarity_distance(stop1[2:], stop2[2:])\n",
    "    \n",
    "    # Combine distances with appropriate weights\n",
    "    combined_distance = coord_weight * coord_distance + similarity_weight * count_similarity\n",
    "    return combined_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_count_stop is a df with columns [' LATITUDE ', ' LONGITUDE ', '1', '2', '3', '4', '5', '6', '7', 'STUD', 'RET', 'WKRS'] \n",
    "data = df_stop_count.values\n",
    "\n",
    "# Custom distance function parameters\n",
    "coord_weight = 0.3\n",
    "similarity_weight = 0.7\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "# Calculate linkage matrix using custom distance function\n",
    "# The custom distance function is a weighted average of the haversine distance between coordinates and the similarity between the stop counts\n",
    "linkage_matrix = linkage(data, method='single', metric=lambda x, y: custom_distance(x, y, coord_weight, similarity_weight))\n",
    "\n",
    "# Create a dendrogram for visualization purposes \n",
    "dendrogram(linkage_matrix, truncate_mode='lastp', p=12, leaf_rotation=90., leaf_font_size=10., show_contracted=True)\n",
    "\n",
    "# Determine clusters based on a desired threshold or number of clusters\n",
    "threshold = 0.4\n",
    "clusters = fcluster(linkage_matrix, t=threshold, criterion='distance', depth=2, R=None, monocrit=None)\n",
    "\n",
    "# print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding cluster labels to the stop count dataframe\n",
    "df_stop_count['Cluster'] = clusters\n",
    "print(df_stop_count.shape)\n",
    "df_stop_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of cluster distribution \n",
    "sns.countplot(x='Cluster', data=df_stop_count, palette='Set1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folium map centered around the mean latitude and longitude\n",
    "center_lat = np.mean(df_stop_count['LATITUDE'])\n",
    "center_lon = np.mean(df_stop_count['LONGITUDE'])\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "\n",
    "cluster_colors = {\n",
    "    1: 'blue',\n",
    "    2: 'red',\n",
    "    3: 'green',\n",
    "    4: 'purple',\n",
    "    5: 'orange',\n",
    "    6: 'darkred',\n",
    "    7: 'lightred',\n",
    "    8: 'beige',\n",
    "    9: 'darkblue',\n",
    "    10: 'darkgreen',\n",
    "    11: 'cadetblue',\n",
    "    12: 'darkpurple',\n",
    "    13: 'lightblue',\n",
    "    14: 'pink',\n",
    "    15: 'lightgreen',\n",
    "    16: 'black',\n",
    "    17: 'lightgray',\n",
    "    18: 'darkgray',\n",
    "    19: 'yellow',\n",
    "    20: 'lightorange'\n",
    "}\n",
    "\n",
    "# Create markers for each stop and color them based on clusters\n",
    "for idx, row in df_stop_count.iterrows():\n",
    "    cluster_color = cluster_colors.get(row['Cluster'], 'gray')  # Default to gray if cluster color is not defined\n",
    "    folium.CircleMarker(location=[row['LATITUDE'], row['LONGITUDE']], radius=5, color=cluster_color).add_to(m)\n",
    "    # Add a label to the marker with the name of the stop obtained by geo-coding the latitude and longitude\n",
    "    # Obtain the name of the stop by geo-coding the latitude and longitude\n",
    "    from geopy.geocoders import Nominatim\n",
    "    geopy = Nominatim(user_agent=\"my-app3\")\n",
    "    location = geopy.reverse(f\"{row['LATITUDE']}, {row['LONGITUDE']}\")\n",
    "    # Color the marker based on the cluster\n",
    "    folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=location.address, icon=folium.Icon(color=cluster_color)).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the map to an HTML file\n",
    "time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "try:\n",
    "    m.save('map/' + file_name + '_clustered_' + time + '.html')\n",
    "except:\n",
    "    # Create a new folder in map folder\n",
    "    os.mkdir('map/' + file_name)\n",
    "    m.save('map/' + file_name + '/' + file_name + '_clustered_' + time + '.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
