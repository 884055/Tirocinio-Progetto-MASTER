{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import folium\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, 'file/carnevale.ipynb')\n",
    "from functions import haversine_distance,euclidean_distance,haversine_distance_normalized,euclidean_distance_normalized,custom_distance, find_csv_files, choose_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = '/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/'\n",
    "dr_data = '/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/data/processed/csvPuliti/'\n",
    "#stops = pd.read_csv(dr+'gtfs/'+'stops.txt', sep=',')\n",
    "#stops.rename(columns={\"stop_id\": \"FERMATA\",\"stop_name\": \"NOME_FERMATA\", \"stop_lat\": \"LATITUDE\", \"stop_lon\": \"LONGITUDE\"}, inplace=True)\n",
    "#cols = ['FERMATA','NOME_FERMATA','LATITUDE','LONGITUDE']\n",
    "#stops = stops[cols]\n",
    "#stops.to_csv(dr+'gtfs/'+ 'stops.csv',index=False)\n",
    "\n",
    "#stops = pd.read_csv(dr+'gtfs/'+ 'stops.csv')\n",
    "#print(f'stops.shape: {stops.shape}')\n",
    "#stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_files(folder_path: str) -> list:\n",
    "    \"\"\"\n",
    "        This function returns a list of all the csv files in the specified folder.\n",
    "        :param folder_path: the path of the folder\n",
    "        :return: a list of all the csv files in the specified folder\n",
    "    \"\"\"\n",
    "\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # File extension is .txt or .csv\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    # Sort the list of txt files in alphabetical order\n",
    "    csv_files.sort()\n",
    "\n",
    "    return csv_files\n",
    "\n",
    "def choose_dataset(txt_files: list) -> str:\n",
    "    \"\"\"\n",
    "        This function returns the path of the txt file chosen by the user.\n",
    "        :param txt_files: the list of txt files\n",
    "        :return: the path of the txt file chosen by the user\n",
    "    \"\"\"\n",
    "    if not txt_files:\n",
    "        print(\"No TXT file found.\")\n",
    "        return \"None\"\n",
    "    if len(txt_files) == 1:\n",
    "        print(\"The following file was found:\")\n",
    "    else:\n",
    "        print(\"The following files were found:\")\n",
    "    for i, file_path in enumerate(txt_files):\n",
    "        print(f\"{i+1}. {file_path}\")\n",
    "    while True:\n",
    "        choice = input(\"Enter the number corresponding to the dataset you wish to use (0 to exit): \")\n",
    "        if not choice.isdigit():\n",
    "            print(\"Enter a valid number.\")\n",
    "            continue\n",
    "        choice = int(choice)\n",
    "        if choice == 0:\n",
    "            return \"None\"\n",
    "        if choice < 1 or choice > len(txt_files):\n",
    "            print(\"Enter a valid number.\")\n",
    "            continue\n",
    "        return txt_files[choice - 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset that has also the geo coordinates\n",
    "# Find all txt files in the data folder\n",
    "csv_file = find_csv_files('/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/data/processed/csvPuliti/')\n",
    "\n",
    "print(\"Select a dataset with geo coordinates from the list:\")\n",
    "\n",
    "# Choose a dataset from the list of txt files\n",
    "selected_dataset = choose_dataset(csv_file)\n",
    "\n",
    "if selected_dataset:\n",
    "    print(f\"You selected the dataset {selected_dataset}\")\n",
    "else:\n",
    "    print(\"No dataset selected.\")\n",
    "\n",
    "path  = selected_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_notebook = 'file/csvPuliti'\n",
    "# dataset_compl = pd.read_csv(dr_data+'dataset_cleaned_tempesportazionePasqua23_part1.csv')\n",
    "\n",
    "dataset_compl = pd.read_csv(path, header=0, sep=',')\n",
    "\n",
    "\n",
    "print(f'dataset_compl.shape: {dataset_compl.shape}')\n",
    "dataset_compl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_compl.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fermate = dataset_compl['FERMATA'].nunique()\n",
    "print(f'Numero di fermate distinte: {num_fermate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_compl_isola_fermate = dataset_compl.copy()\n",
    "\n",
    "#dataset_compl_isola_fermate = dataset_compl_isola_fermate[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_compl['DATA_VALIDAZIONE'] = pd.to_datetime(dataset_compl['DATA_VALIDAZIONE'])\n",
    "dataset_compl['DATA'] = pd.to_datetime(dataset_compl['DATA'])\n",
    "#dataset_compl.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the interval of dates for which we have data\n",
    "print(f'Date range: {dataset_compl.DATA_VALIDAZIONE.min()} to {dataset_compl.DATA_VALIDAZIONE.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_clust = dataset_compl.copy()\n",
    "#dataset_clust.drop(columns=['DATA', 'ORA','DESCRIZIONE_TITOLO'],inplace=True)\n",
    "dataset_clust.drop(columns=['DESCRIZIONE_TITOLO'],inplace=True)\n",
    "dataset_clust.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_clust.TICKET_CODE.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 5-STUD, 6-STUD to STUD in the dataframe\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].replace(['5-STUD', '6-STUD'], 'STUD')\n",
    "# Change 5-WKRS, 6-WKRS to WKRS in the dataframe\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].replace(['5-WKRS', '6-WKRS'], 'WKRS')\n",
    "# Change 5-RET, 6-RET to RET in the dataframe\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].replace(['5-RET', '6-RET'], 'RET')\n",
    "# Change 5, 6 to LOC (that means locals) in the dataframe\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].replace(['5', '6'], 'LOC')\n",
    "# Change 1, 2, 3, 4 to tuorist TUR in the dataframe\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].replace(['1', '2', '3', '4'], 'TUR')\n",
    "\n",
    "# Change RET to LOC in the dataframe\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].replace(['RET'], 'LOC')\n",
    "\n",
    "# Remove the rows with ticket code 7 (75 minutes ticket - overrappresented) and WKRS (workers - underrepresented)\n",
    "dataset_clust = dataset_clust[dataset_clust.TICKET_CODE != '7']\n",
    "dataset_clust = dataset_clust[dataset_clust.TICKET_CODE != 'WKRS']\n",
    "\n",
    "\n",
    "\n",
    "# Print the unique ticket codes\n",
    "# Print information about the changes made\n",
    "print('The ticket codes 5-STUD and 6-STUD have been changed to STUD')\n",
    "print('The ticket codes 5-WKRS and 6-WKRS have been changed to WKRS')\n",
    "print('The ticket codes 5-RET and 6-RET have been changed to RET')\n",
    "print('The ticket codes 5 and 6 have been changed to LOC')\n",
    "print('The ticket codes 1, 2, 3 and 4 have been changed to TUR')\n",
    "print('The ticket code RET has been changed to LOC')\n",
    "print('The ticket codes 7 (75 minutes ticket - overrappresented) and WKRS (workers - underrepresented) have been removed')\n",
    "\n",
    "# Convert all the ticket codes to string\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].astype(str)\n",
    "\n",
    "ticket_codes = dataset_clust['TICKET_CODE'].unique()\n",
    "# Sort the ticket codes\n",
    "ticket_codes.sort()\n",
    "\n",
    "print('The considered ticket codes are: ', ticket_codes)\n",
    "\n",
    "# Convert all the ticket codes to string\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tickets = {'LOC': 'Ticket for resident', 'STUD': 'Ticket for students',\n",
    "                'TUR': 'Ticket for tourist'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a pie chart of the column 'TICKET_CODE'\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "# Change palette to have more more colors\n",
    "sns.set_palette(\"tab20\")\n",
    "dataset_clust['TICKET_CODE'].value_counts().sort_index().plot.pie(startangle=90)\n",
    "\n",
    "plt.title('Date range: ' + str(dataset_clust['DATA'].min().to_pydatetime().date()) + ' - ' + str(dataset_clust['DATA'].max().to_pydatetime().date()), fontsize=25)\n",
    "plt.ylabel('')\n",
    "\n",
    "# Modify the legend to have the description of the ticket profile and the percentage of each category appended\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "labels = [s + ' - {:.5f}%'.format(100*v/len(dataset_clust)) for s, v in zip(dataset_clust['TICKET_CODE'].value_counts().sort_index().rename(dict_tickets).index, dataset_clust['TICKET_CODE'].value_counts().sort_index())]\n",
    "ax.legend(handles, labels, loc='center left', bbox_to_anchor=(1, 0, 0.5, 1), fontsize=22)\n",
    "\n",
    "# Ensure that the labels are not overlapped on the pie chart\n",
    "plt.tight_layout()\n",
    "\n",
    "# Remove the labels on the pie chart\n",
    "texts = [text for text in ax.texts]\n",
    "for text in texts:\n",
    "    text.set_visible(False)\n",
    "\n",
    "fig.patch.set_facecolor('white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fermate = dataset_clust['FERMATA'].nunique()\n",
    "print(f'Numero di fermate distinte: {num_fermate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering in base ai ticket codes e rispetto alla vicinanza geografica delle celle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of stops\n",
    "df_stop = dataset_clust.copy()\n",
    "#df_stop = df_stop[['LATITUDE', 'LONGITUDE', 'CLUSTER','FERMATA']]\n",
    "print(f'df_stop.shape: {df_stop.shape}')\n",
    "df_stop.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_count = df_stop.groupby(['LATITUDE', 'LONGITUDE', 'TICKET_CODE', 'FERMATA', 'DESCRIZIONE']).size().reset_index(name='COUNT')\n",
    "\n",
    "print(f'df_stop_count.shape: {df_stop_count.shape}')\n",
    "df_stop_count.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the column COUNT\n",
    "df_stop_count['COUNT'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_count.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for the ticket codes but maintaining the information about the stops\n",
    "df_stop_count = df_stop_count.pivot_table(index=['LATITUDE', 'LONGITUDE', 'FERMATA', 'DESCRIZIONE'], columns='TICKET_CODE', values='COUNT', fill_value=0)\n",
    "df_stop_count.reset_index(inplace=True)\n",
    "\n",
    "# For each stop (LATITUDE, LONGITUDE), change the counter of each ticket code as a percentage of the total number of tickets\n",
    "for row in range(len(df_stop_count)):\n",
    "    total = df_stop_count.iloc[row, 4:].sum()\n",
    "    for col in range(4, len(df_stop_count.columns)):\n",
    "        df_stop_count.iloc[row, col] = df_stop_count.iloc[row, col] / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_stop_count.at[0, 'TUR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_locations = len(df_stop_count)\n",
    "print(f'num_locations: {num_locations}')\n",
    "haversine_matrix = np.zeros((num_locations, num_locations))\n",
    "euclidean_distances = np.zeros((num_locations, num_locations))\n",
    "\n",
    "for i in range(num_locations):\n",
    "    for j in range(i + 1, num_locations):\n",
    "        coord1 = (df_stop_count.at[i, 'LATITUDE'], df_stop_count.at[i, 'LONGITUDE'])\n",
    "        coord2 = (df_stop_count.at[j, 'LATITUDE'], df_stop_count.at[j, 'LONGITUDE'])\n",
    "        distance = haversine_distance(coord1, coord2)\n",
    "        haversine_matrix[i][j] = distance\n",
    "        haversine_matrix[j][i] = distance  # Since distance is symmetric\n",
    "        \n",
    "        stops1 = [df_stop_count.at[i, 'LOC'], df_stop_count.at[i, 'STUD'], df_stop_count.at[i, 'TUR']]\n",
    "        stops2 = [df_stop_count.at[j, 'LOC'], df_stop_count.at[j, 'STUD'], df_stop_count.at[j, 'TUR']]\n",
    "        \n",
    "    \n",
    "        stops1 = np.array(stops1)\n",
    "        stops2 = np.array(stops2)\n",
    "\n",
    "        euclidean_dist = euclidean_distance(stops1, stops2)\n",
    "        euclidean_distances[i][j] = euclidean_dist\n",
    "        euclidean_distances[j][i] = euclidean_dist  # Since distance is symmetric\n",
    "        \n",
    "\n",
    "# Create a DataFrame to store the haversine distances\n",
    "haversine_df = pd.DataFrame(haversine_matrix, index=df_stop_count.index, columns=df_stop_count.index)\n",
    "\n",
    "# Find the minimum and maximum haversine distances\n",
    "min_distance = haversine_df.values.min()\n",
    "max_distance = haversine_df.values.max()\n",
    "avg_euclidean_distance = euclidean_distances.mean()\n",
    "\n",
    "\n",
    "# Print the haversine distance DataFrame and the minimum and maximum distances\n",
    "#print(\"Haversine Distance DataFrame:\")\n",
    "#print(haversine_df)\n",
    "\n",
    "print(\"Minimum Haversine Distance:\", min_distance, \"km\")\n",
    "print(\"Maximum Haversine Distance:\", max_distance, \"km\")\n",
    "\n",
    "\n",
    "# Create a DataFrame to store the haversine distances\n",
    "euclidean_df = pd.DataFrame(euclidean_distances, index=df_stop_count.index, columns=df_stop_count.index)\n",
    "\n",
    "# Find the minimum and maximum Euclidean distances\n",
    "min_euclidean_distance = euclidean_distances.min()\n",
    "max_euclidean_distance = euclidean_distances.max()\n",
    "\n",
    "# Print the haversine distance DataFrame and the minimum and maximum distances\n",
    "#print(\"Haversine Distance DataFrame:\")\n",
    "#print(haversine_df)\n",
    "\n",
    "# Now you have the normalized Euclidean distances in the range [0, 1]\n",
    "print(\"Minimum Euclidean Distance:\", min_euclidean_distance.min())\n",
    "print(\"Maximum Euclidean Distance:\", max_euclidean_distance.max())\n",
    "print(\"Average Euclidean Distance:\", avg_euclidean_distance/max_euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min(min_euclidean_distance), max(max_euclidean_distance)\n",
    "max_euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From df_stop_count remove descrizione\n",
    "df_stop_count_data = df_stop_count.copy()\n",
    "df_stop_count_data = df_stop_count_data.drop(columns=['FERMATA','DESCRIZIONE'])\n",
    "\n",
    "data = df_stop_count_data.values\n",
    "print(data.shape)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop 1: 5102.0, stop2: 5103.0, combined_distance: 0.0396467042622267\n",
    "#stop 1: 5102.0, stop2: -5.0, combined_distance: 0.22699456069457805\n",
    "#stop 1: 5102.0, stop2: -2.0, combined_distance: 0.34764672602143193\n",
    "\n",
    "#stop 1: 5050.0, stop2: 5049.0, combined_distance: 0.0017922304598963794 --> stesso cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Custom distance function parameters\n",
    "coord_weight = 0.4\n",
    "# coord_weight = 0\n",
    "similarity_weight = 1-coord_weight\n",
    "print(f'coord_weight: {coord_weight}, similarity_weight: {similarity_weight}')\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "# Calculate linkage matrix using custom distance function\n",
    "# The custom distance function is a weighted average of the haversine distance between coordinates and \n",
    "# the similarity between the stop counts\n",
    "linkage_matrix = linkage(data, method='complete', \n",
    "                         metric=lambda x, y: custom_distance(x, y, coord_weight, similarity_weight,min_distance,\n",
    "                                                             max_distance,min_euclidean_distance,\n",
    "                                                             max_euclidean_distance))\n",
    "\n",
    "\n",
    "\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 20))\n",
    "plt.title('Hierarchical Clustering Dendrogram', fontsize=20)\n",
    "plt.xlabel('cluster', fontsize=20)\n",
    "plt.ylabel('distance', fontsize=20)\n",
    "# Create a dendrogram for visualization purposes \n",
    "# y-ticks must contain 0.1\n",
    "plt.yticks(np.arange(0, 1.1, 0.1))\n",
    "dendrogram(linkage_matrix, #truncate_mode='lastp', #p=12, \n",
    "           #orientation=\"right\",\n",
    "           leaf_rotation=90., leaf_font_size=12.)#, show_contracted=True)\n",
    "# Show threshold 0.1 \n",
    "plt.axhline(y=0.1, color='magenta', linestyle='--', linewidth=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_stop_count[(df_stop_count['FERMATA']==5103) | (df_stop_count['FERMATA']==-2)]\n",
    "#stop 1: 5103.0, stop2: -2.0, combined_distance: 36.09851949056943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5097 torcello\n",
    "# 5072 verso chioggia \n",
    "# -2 piazzale roma\n",
    "\n",
    "#dataset_time_slots[dataset_time_slots['FERMATA'] == -5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine clusters based on a desired threshold or number of clusters\n",
    "threshold = 0.1\n",
    "# threshold = 0.15\n",
    "clusters = fcluster(linkage_matrix, t=threshold, criterion='distance', depth=2, R=None, monocrit=None)\n",
    "# print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_hd = min(haversine_distance_arr)\n",
    "#max_hd = max(haversine_distance_arr)\n",
    "#avg_hd = np.average(haversine_distance_arr)\n",
    "\n",
    "#min_ed = min(eu_similarity_arr)\n",
    "#max_ed = max(eu_similarity_arr)\n",
    "#avg_ed = np.average(eu_similarity_arr)\n",
    "\n",
    "#print(f'min_hd: {min_hd}, max_hd: {max_hd}, avg_hd: {avg_hd}')\n",
    "#print(f'min_ed: {min_ed}, max_ed: {max_ed}, avg_ed: {avg_ed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(0.7*0.16)\n",
    "#print(0.3*0.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding cluster labels to the stop count dataframe\n",
    "df_stop_count['CLUSTER'] = clusters\n",
    "print(df_stop_count.shape)\n",
    "df_stop_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_stop_count[df_stop_count['CLUSTER']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cl = ['FERMATA','CLUSTER']\n",
    "df_cl = df_stop_count.copy()\n",
    "df_cl = df_cl[cols_cl]\n",
    "\n",
    "dataset_compl = dataset_compl.merge(df_cl, on=['FERMATA'])\n",
    "\n",
    "df_sum_validations = dataset_compl.copy()\n",
    "df_sum_validations = dataset_compl.groupby(['CLUSTER']).size().reset_index()\n",
    "df_sum_validations.rename(columns={0: 'n_V'},inplace=True)\n",
    "df_sum_validations.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'DESCRIZIONE' not in df_stop_count.columns :\n",
    "    df_stop_count = df_stop_count.merge(fermate, on=['LATITUDE','LONGITUDE','FERMATA'])\n",
    "#df_stop_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hex codes of tab_20 palette in the format 1:hex_code dict\n",
    "cluster_colors = dict(zip(range(1,51), sns.color_palette(\"tab20\", 50).as_hex()))\n",
    "#cluster_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of cluster distribution \n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='CLUSTER', data=df_stop_count)#, palette=cluster_colors)\n",
    "plt.yticks(np.arange(0, max(df_stop_count['CLUSTER'].value_counts()) + 5, 2))\n",
    "plt.title('Cluster Distribution: ' + 'date range: ' + str(df_stop['DATA'].min().date()) + ' to ' \n",
    "          + str(df_stop['DATA'].max().date()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_stop_count.copy()\n",
    "cols_grouped = ['CLUSTER', 'LOC', 'STUD', 'TUR']\n",
    "grouped = grouped[cols_grouped]\n",
    "grouped.reset_index(inplace=True,drop=True)\n",
    "grouped = grouped.rename_axis(None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mean = grouped.groupby(['CLUSTER']).mean()\n",
    "grouped_mean.reset_index(inplace=True)\n",
    "grouped_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è la standard deviation: se vuoi la varianza togli sqrt\n",
    "grouped_var = grouped.groupby(['CLUSTER']).apply(lambda x: np.sqrt(np.var(x)))\n",
    "grouped_var.drop(columns=['CLUSTER'],inplace = True)\n",
    "grouped_var.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = grouped_mean.merge(grouped_var, on=['CLUSTER'])\n",
    "grouped = grouped.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = grouped.round(decimals = 3)\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ticket codes: 1, 2, 3, 4, 7, LOC, RET, STUD, WKRS\n",
    "list_tc = ['LOC', 'STUD', 'TUR']\n",
    "for i in list_tc:\n",
    "    col_mean = str(i) + '_x'\n",
    "    col_var = str(i) + '_y'\n",
    "    col_ts = 'Mean_Var_' + str(i)\n",
    "    grouped[col_ts] = grouped[col_mean].astype(str) + ' ± ' + grouped[col_var].apply(lambda x: f'{x:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all columns of the dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped.shape)\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_final = ['CLUSTER','Mean_Var_LOC','Mean_Var_STUD','Mean_Var_TUR']\n",
    "grouped = grouped[cl_final]\n",
    "grouped = grouped.merge(df_sum_validations, on=['CLUSTER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by CLUSTER and aggregate the 'DESCRIZIONE' column into a list\n",
    "clustered_stops = df_stop_count.groupby('CLUSTER')['DESCRIZIONE'].agg(['count', list]).reset_index()\n",
    "clustered_stops = clustered_stops.merge(grouped, on = ['CLUSTER'])\n",
    "#for i in range(6) : \n",
    "#    clustered_stops[i] = clustered_stops[i].round(decimals = 3)\n",
    "#clustered_stops.sort_values(by=[0,1,2,3,4,5])\n",
    "clustered_stops.rename(columns={'count': 'n_F', 'list': 'STOPS'},inplace=True)\n",
    "clustered_cols = ['CLUSTER', 'n_F', 'n_V', 'Mean_Var_LOC', 'Mean_Var_STUD', 'Mean_Var_TUR', 'STOPS']\n",
    "\n",
    "clustered_stops = clustered_stops[clustered_cols]\n",
    "clustered_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #df_stop_count[df_stop_count['CLUSTER']==21]\n",
    "# grouped = df_stop_count.copy()\n",
    "# cols_grouped = ['CLUSTER', '1', '2', '3', '4', '7', 'LOC', 'RET', 'STUD', 'WKRS']\n",
    "# grouped = grouped[cols_grouped]\n",
    "# grouped.reset_index(inplace=True,drop=True)\n",
    "# grouped = grouped.rename_axis(None, axis=1)\n",
    "# #grouped\n",
    "\n",
    "# grouped_mean = grouped.groupby(['CLUSTER']).mean()\n",
    "# grouped_mean.reset_index(inplace=True)\n",
    "# grouped_mean\n",
    "\n",
    "# # è la standard deviation: se vuoi la varianza togli sqrt\n",
    "# grouped_var = grouped.groupby(['CLUSTER']).apply(lambda x: np.sqrt(np.var(x)))\n",
    "# grouped_var.drop(columns=['CLUSTER'],inplace = True)\n",
    "# grouped_var.reset_index(inplace=True)\n",
    "\n",
    "# grouped = grouped_mean.merge(grouped_var, on=['CLUSTER'])\n",
    "# grouped = grouped.fillna(0)\n",
    "# #grouped\n",
    "\n",
    "# grouped = grouped.round(decimals = 3)\n",
    "# grouped\n",
    "# grouped\n",
    "\n",
    "# for i in range(6) :  \n",
    "#     col_mean = str(i) + '_x'\n",
    "#     col_var = str(i) + '_y'\n",
    "#     col_ts = 'Mean_Var_' + str(i)\n",
    "#     grouped[col_ts] = grouped[col_mean].astype(str) + ' ± ' + grouped[col_var].apply(lambda x: f'{x:.2f}')\n",
    "\n",
    "# cl_final = ['CLUSTER','Mean_Var_0','Mean_Var_1','Mean_Var_2','Mean_Var_3','Mean_Var_4','Mean_Var_5']\n",
    "# grouped = grouped[cl_final]\n",
    "\n",
    "# #grouped\n",
    "\n",
    "# # Group by CLUSTER and aggregate the 'DESCRIZIONE' column into a list\n",
    "# clustered_stops = df_stop_count.groupby('CLUSTER')['DESCRIZIONE'].agg(['count', list]).reset_index()\n",
    "# clustered_stops = clustered_stops.merge(grouped, on = ['CLUSTER'])\n",
    "# #for i in range(6) : \n",
    "# #    clustered_stops[i] = clustered_stops[i].round(decimals = 3)\n",
    "# #clustered_stops.sort_values(by=[0,1,2,3,4,5])\n",
    "# clustered_stops.rename(columns={'count': 'NUM', 'list': 'STOPS'},inplace=True)\n",
    "# clustered_cols = ['CLUSTER', 'NUM', 'Mean_Var_0', 'Mean_Var_1', 'Mean_Var_2','Mean_Var_3', 'Mean_Var_4',\n",
    "#                   'Mean_Var_5','STOPS']\n",
    "# clustered_stops = clustered_stops[clustered_cols]\n",
    "# clustered_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a folium map centered around the mean latitude and longitude\n",
    "center_lat = np.mean(df_stop_count['LATITUDE'])\n",
    "center_lon = np.mean(df_stop_count['LONGITUDE'])\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "\n",
    "# Create markers for each stop and color them based on clusters\n",
    "for idx, row in df_stop_count.iterrows():\n",
    "    # Retrieve Cluster id and set it as a integer\n",
    "    cluster = int(row['CLUSTER'])\n",
    "\n",
    "    cluster_color = cluster_colors.get(cluster, 'gray')  # Default to gray if cluster color is not defined\n",
    "    #print(cluster_color)\n",
    "\n",
    "    # cluster_color = cluster_\n",
    "    # colors.get(cluster, 'gray')  # Default to gray if cluster color is not defined\n",
    "    # Add a label to the marker with the name of the stop contained in the column 'DESCRIZIONE'\n",
    "    # Retrieve the name of the stop from the column 'DESCRIZIONE' of the dataframe df matching the FERMATA\n",
    "    location = df_stop_count[df_stop_count['FERMATA'] == row['FERMATA']]['DESCRIZIONE'].values[0]\n",
    "    # Retrieve FERMATA id and set it as a integer\n",
    "    fermata = int(row['FERMATA'])\n",
    "    # Retrieve the total number of validations for the stop\n",
    "    tot_validations = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'n_V'].iloc[0]\n",
    "    # Retrieve the total number of stops for the stop\n",
    "    tot_stops = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'n_F'].iloc[0]\n",
    "    # Retrieve Cluster id and set it as a integer\n",
    "    cluster = int(row['CLUSTER'])\n",
    "    mean_loc = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_LOC'].iloc[0]\n",
    "    mean_stud = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_STUD'].iloc[0]\n",
    "    mean_tur = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_TUR'].iloc[0]\n",
    "    # Create a string for popup message containing the name of the stop, FERMATA and the cluster in BOLD\n",
    "    #popup_message = f\"<b><i>{location}</i></b><br><br><u>Fermata:</u> {fermata}<br><u>Cluster:</u> {cluster}<br><u>Mean and sd:</u> {mean_0}<br>\"\n",
    "    popup=folium.Popup(f\"<b><i>{location}</i></b><br><br><u>Fermata:</u> {fermata}<br><u>Cluster:</u> {cluster}<br><u>Tot stops:</u> {tot_stops}<br><u>Tot validations:</u> {tot_validations}<br><u>Mean and sd:</u> <i>Mean LOC:</i> {mean_loc}<br><i>Mean STUD:</i> {mean_stud}<br><i>Mean TUR:</i> {mean_tur}<br>\", max_width=300)\n",
    "\n",
    "\n",
    "    # Add a marker to the map with the popup message and the color of the cluster\n",
    "    # folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup_message, icon=folium.Icon(color=cluster_color)).add_to(m)\n",
    "    # Add a marker to the map with the popup message and the color of the cluster\n",
    "    # folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup_message, icon=folium.Icon(color=cluster_color)).add_to(m)\n",
    "    if fermata < 0:\n",
    "        folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup, \n",
    "                      icon=folium.Icon(color='white', icon_color=cluster_color, icon='bus', prefix='fa')).add_to(m)\n",
    "    else:\n",
    "        folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup, \n",
    "                      icon=folium.Icon(color='white', icon_color=cluster_color, icon='ship', prefix='fa')).add_to(m)\n",
    "\n",
    "# Add layer control and show map\n",
    "#m.add_child(folium.LayerControl(collapsed=False))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export the map to an HTML file\n",
    "\n",
    "# th = str(threshold)\n",
    "# th = th.replace('.', '')\n",
    "# print(f'threshold: {th}')\n",
    "# num_cl = str(df_stop_count.CLUSTER.max())\n",
    "# print(f'number of clusters: {num_cl}')\n",
    "\n",
    "# file_name = 'finePrimavera'\n",
    "# dr_map = 'Documenti/risultati clustering/Nuovo_STUD-LOC-TUR/' + file_name + '/'\n",
    "# #time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# try:\n",
    "#     m.save(dr+dr_map + file_name + '_threshold_' + th + '_' + num_cl + 'cluster' +'.html')\n",
    "# except:\n",
    "#     # Create a new folder in map folder\n",
    "#     os.mkdir(dr+dr_map + file_name)\n",
    "#     m.save(dr+dr_map + file_name + '_threshold_' + th + '_' + num_cl + 'cluster' +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Map of cluster 7\n",
    "# df_cluster_7 = df_stop_count[df_stop_count['CLUSTER'] == 12]\n",
    "# df_cluster_7.reset_index(inplace=True, drop=True)\n",
    "# # Create a folium map centered around the mean latitude and longitude\n",
    "# center_lat = np.mean(df_cluster_7['LATITUDE'])\n",
    "# center_lon = np.mean(df_cluster_7['LONGITUDE'])\n",
    "# m = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "\n",
    "# # Create markers for each stop and color them based on clusters\n",
    "# for idx, row in df_cluster_7.iterrows():\n",
    "#     # Retrieve Cluster id and set it as a integer\n",
    "#     cluster = int(row['CLUSTER'])\n",
    "\n",
    "#     cluster_color = cluster_colors.get(cluster, 'gray')  # Default to gray if cluster color is not defined\n",
    "#     #print(cluster_color)\n",
    "\n",
    "#     # cluster_color = cluster_\n",
    "#     # colors.get(cluster, 'gray')  # Default to gray if cluster color is not defined\n",
    "#     # Add a label to the marker with the name of the stop contained in the column 'DESCRIZIONE'\n",
    "#     # Retrieve the name of the stop from the column 'DESCRIZIONE' of the dataframe df matching the FERMATA\n",
    "#     location = df_cluster_7[df_cluster_7['FERMATA'] == row['FERMATA']]['DESCRIZIONE'].values[0]\n",
    "#     # Retrieve FERMATA id and set it as a integer\n",
    "#     fermata = int(row['FERMATA'])\n",
    "#     # Retrieve the total number of validations for the stop\n",
    "#     tot_validations = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'n_V'].iloc[0]\n",
    "#     # Retrieve the total number of stops for the stop\n",
    "#     tot_stops = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'n_F'].iloc[0]\n",
    "#     # Retrieve Cluster id and set it as a integer\n",
    "#     cluster = int(row['CLUSTER'])\n",
    "#     mean_loc = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_LOC'].iloc[0]\n",
    "#     mean_stud = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_STUD'].iloc[0]\n",
    "#     mean_tur = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_TUR'].iloc[0]\n",
    "#     # Create a string for popup message containing the name of the stop, FERMATA and the cluster in BOLD\n",
    "#     #popup_message = f\"<b><i>{location}</i></b><br><br><u>Fermata:</u> {fermata}<br><u>Cluster:</u> {cluster}<br><u>Mean and sd:</u> {mean_0}<br>\"\n",
    "#     popup=folium.Popup(f\"<b><i>{location}</i></b><br><br><u>Fermata:</u> {fermata}<br><u>Cluster:</u> {cluster}<br><u>Tot stops:</u> {tot_stops}<br><u>Tot validations:</u> {tot_validations}<br><u>Mean and sd:</u> <i>Mean LOC:</i> {mean_loc}<br><i>Mean STUD:</i> {mean_stud}<br><i>Mean TUR:</i> {mean_tur}<br>\", max_width=300)\n",
    "\n",
    "#     if fermata < 0:\n",
    "#         folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup, \n",
    "#                       icon=folium.Icon(color='white', icon_color=cluster_color, icon='bus', prefix='fa')).add_to(m)\n",
    "#     else:\n",
    "#         folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup, \n",
    "#                       icon=folium.Icon(color='white', icon_color=cluster_color, icon='ship', prefix='fa')).add_to(m)\n",
    "        \n",
    "# # Add layer control and show map\n",
    "# #m.add_child(folium.LayerControl(collapsed=False))\n",
    "# m\n",
    "\n",
    "# # export map\n",
    "# m.save('map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of unique values of the column 'FERMATA'\n",
    "print('Number of unique values of the column FERMATA: {}'.format(df_stop_count['FERMATA'].nunique()))\n",
    "# print the numer of clusters\n",
    "print('Number of clusters: {}'.format(df_stop_count['CLUSTER'].nunique()))\n",
    "# Print the average number of stops per cluster\n",
    "print('Average number of stops per cluster: {}'.format(df_stop_count['CLUSTER'].value_counts().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as pex\n",
    "\n",
    "# source_indices = [df_stop_count[df_stop_count['FERMATA'] == row['FERMATA']].index[0] for idx, row in df_stop_count.iterrows()]\n",
    "# target_indices = [df_stop_count[df_stop_count['CLUSTER'] == row['CLUSTER']].index[0] for idx, row in df_stop_count.iterrows()]\n",
    "\n",
    "\n",
    "# sorted_clusters = sorted(df_stop_count['CLUSTER'].unique())\n",
    "# cluster_index_mapping = {cluster: index for index, cluster in enumerate(sorted_clusters)}\n",
    "# target_indices = [cluster_index_mapping[row['CLUSTER']] for idx, row in df_stop_count.iterrows()]\n",
    "\n",
    "\n",
    "# colors = pex.colors.qualitative.D3 ## Color Pallete\n",
    "\n",
    "# node_colors_mappings = dict([(node, np.random.choice(colors)) for node in df_stop_count['CLUSTER']])\n",
    "\n",
    "# node_colors = [node_colors_mappings[node] for node in df_stop_count['CLUSTER']]\n",
    "# edge_colors = [node_colors_mappings[node] for node in df_stop_count['CLUSTER']]\n",
    "\n",
    "# plt.figure(figsize=(20, 40))\n",
    "# # Sankey diagram ORDERED BY CLUSTER\n",
    "# fig = go.Figure(data=[go.Sankey(\n",
    "#     node = dict(\n",
    "#         pad = 15,\n",
    "#         thickness = 20,\n",
    "#         line = dict(color = \"black\", width = 0.5),\n",
    "#         label = sorted_clusters,\n",
    "#         color = node_colors\n",
    "#     ),\n",
    "#     link = dict(\n",
    "#         source = source_indices,\n",
    "#         target = target_indices,\n",
    "#         value = df_stop_count['FERMATA'],\n",
    "#         color = edge_colors\n",
    "#     ))])\n",
    "\n",
    "\n",
    "# fig.update_layout(title_text=\"Basic Sankey Diagram\", font_size=10)\n",
    "\n",
    "# # get the image more readable\n",
    "# fig.update_layout(width=1000, height=2000)\n",
    "\n",
    "# # Add information about the number of stops and the number of validations\n",
    "# fig.update_layout(title_text=\"Sankey Diagram: \" + 'date range: ' + str(df_stop['DATA'].min().date()) + ' to '\n",
    "#                     + str(df_stop['DATA'].max().date()) + '<br>' + 'Number of stops: ' + str(df_stop_count['FERMATA'].nunique()) + '<br>' + 'Number of clusters: ' + str(df_stop_count['CLUSTER'].nunique()) + '<br>' + 'Average number of stops per cluster: ' + str(df_stop_count['CLUSTER'].value_counts().mean()) + '<br>' + 'Average number of validations per cluster: ' + str(df_stop_count['CLUSTER'].value_counts().mean()), font_size=10)\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent in a diagram the average and the standard deviation of LOC, STUD and TUR for each cluster\n",
    "# Create a dataframe with the columns 'CLUSTER', 'Mean_LOC', 'Mean_STUD', 'Mean_TUR', 'Std_LOC', 'Std_STUD', 'Std_TUR'\n",
    "df_mean = grouped.copy()\n",
    "df_mean_std = df_mean[['CLUSTER', 'Mean_Var_LOC', 'Mean_Var_STUD', 'Mean_Var_TUR']]\n",
    "df_mean_std.rename(columns={'Mean_Var_LOC': 'Mean_LOC', 'Mean_Var_STUD': 'Mean_STUD', 'Mean_Var_TUR': 'Mean_TUR'}, inplace=True)\n",
    "# Convert 'Mean_Var_LOC', 'Mean_Var_STUD', and 'Mean_Var_TUR' columns to float\n",
    "# Extract and convert the Mean and Std from 'Mean_Var_LOC', 'Mean_Var_STUD', and 'Mean_Var_TUR' columns\n",
    "df_mean_std['Std_LOC'] = grouped['Mean_Var_LOC'].apply(lambda x: float(x.split(' ± ')[1]))\n",
    "df_mean_std['Std_STUD'] = grouped['Mean_Var_STUD'].apply(lambda x: float(x.split(' ± ')[1]))\n",
    "df_mean_std['Std_TUR'] = grouped['Mean_Var_TUR'].apply(lambda x: float(x.split(' ± ')[1]))\n",
    "df_mean_std['Mean_LOC'] = grouped['Mean_Var_LOC'].apply(lambda x: float(x.split(' ± ')[0]))\n",
    "df_mean_std['Mean_STUD'] = grouped['Mean_Var_STUD'].apply(lambda x: float(x.split(' ± ')[0]))\n",
    "df_mean_std['Mean_TUR'] = grouped['Mean_Var_TUR'].apply(lambda x: float(x.split(' ± ')[0]))\n",
    "\n",
    "\n",
    "\n",
    "df_mean_std.head()\n",
    "\n",
    "# Represent in a diagram the average and the standard deviation of LOC, STUD and TUR for each cluster\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.errorbar(df_mean_std['CLUSTER'], df_mean_std['Mean_LOC'], yerr=df_mean_std['Std_LOC'], fmt='o', label='LOC')\n",
    "plt.errorbar(df_mean_std['CLUSTER'], df_mean_std['Mean_STUD'], yerr=df_mean_std['Std_STUD'], fmt='o', label='STUD')\n",
    "plt.errorbar(df_mean_std['CLUSTER'], df_mean_std['Mean_TUR'], yerr=df_mean_std['Std_TUR'], fmt='o', label='TUR')\n",
    "plt.xlabel('Cluster', fontsize=15)\n",
    "plt.ylabel('Mean and standard deviation', fontsize=15)\n",
    "\n",
    "# x-ticks from 1 to the number of clusters\n",
    "plt.xticks(np.arange(1, df_mean_std['CLUSTER'].max() + 1, 1))\n",
    "\n",
    "# y-ticks from 0 to 100\n",
    "plt.yticks(np.arange(0, 102, 2))\n",
    "\n",
    "# Add grid with opacity 0.4\n",
    "plt.grid(alpha=0.2)\n",
    "\n",
    "plt.title('Mean and standard deviation of LOC, STUD and TUR for each cluster', fontsize=20)\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Print information about this diagram\n",
    "print('The diagram shows the average and the standard deviation of LOC, STUD and TUR for each cluster')\n",
    "print('The average and the standard deviation of LOC, STUD and TUR are calculated on the percentage of validations for each ticket code')\n",
    "\n",
    "# How to read the diagram:\n",
    "# The diagram shows the average and the standard deviation of LOC, STUD and TUR for each cluster\n",
    "# The average and the standard deviation of LOC, STUD and TUR are calculated on the percentage of validations for each ticket code\n",
    "# For example, for cluster 1, the average percentage of validations for LOC is 0.0% with a standard deviation of 0.0%\n",
    "# the point is the average and the error bar is the standard deviation of the percentage of validations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the map to an HTML file\n",
    "\n",
    "th = str(threshold)\n",
    "th = th.replace('.', '')\n",
    "print(f'threshold: {th}')\n",
    "num_cl = str(df_stop_count.CLUSTER.max())\n",
    "print(f'number of clusters: {num_cl}')\n",
    "\n",
    "file_name = 'carnevale_entire'\n",
    "dr_map = 'Documenti/risultati clustering/Nuovo_STUD-LOC-TUR/' + file_name + '/'\n",
    "#time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "try:\n",
    "    m.save(dr+dr_map + file_name + '_threshold_' + th + '_' + num_cl + 'cluster' +'.html')\n",
    "except:\n",
    "    # Create a new folder in map folder\n",
    "    os.mkdir(dr+dr_map + file_name)\n",
    "    m.save(dr+dr_map + file_name + '_threshold_' + th + '_' + num_cl + 'cluster' +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Document saved in: ' + dr+dr_map + file_name + '_threshold_' + th + '_' + num_cl + 'cluster' +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For each Cluster id, print the number of stops in that cluster, the FERMATA id and the name of the stop\n",
    "df_stop_count_copy = df_stop_count.copy()\n",
    "# To df_stop_count add a column with the name of the stop matching the FERMATA\n",
    "#df_stop_count_copy['DESCRIZIONE'] = df_stop_count_copy['FERMATA'].\n",
    "#apply(lambda x: df[df['FERMATA'] == x]['DESCRIZIONE'].values[0])\n",
    "\n",
    "# Consider only the columns ['Cluster', 'FERMATA', 'DESCRIZIONE']\n",
    "df_stop_count_copy = df_stop_count_copy[['CLUSTER', 'FERMATA', 'DESCRIZIONE']]\n",
    "df_stop_count_copy.columns.name = None\n",
    "\n",
    "#df_stop_count_copy.groupby(['CLUSTER'])\n",
    "#df_stop_count_copy = df_stop_count_copy.set_index(['CLUSTER','FERMATA'])\n",
    "df_stop_count_copy\n",
    "\n",
    "\n",
    "print('Number of clusters: ', max(df_stop_count_copy['CLUSTER']))\n",
    "print('\\n')\n",
    "for cluster in range(1, max(df_stop_count_copy['CLUSTER']) + 1):\n",
    "    print(f\"CLUSTER {cluster} contains {len(df_stop_count_copy[df_stop_count_copy['CLUSTER'] == cluster])} stops.\")\n",
    "    print(df_stop_count_copy[df_stop_count_copy['CLUSTER'] == cluster][['FERMATA', 'DESCRIZIONE']])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map of cluster \n",
    "df_cluster_7 = df_stop_count[df_stop_count['CLUSTER'] == 7]\n",
    "df_cluster_7.reset_index(inplace=True, drop=True)\n",
    "# Create a folium map centered around the mean latitude and longitude\n",
    "center_lat = np.mean(df_cluster_7['LATITUDE'])\n",
    "center_lon = np.mean(df_cluster_7['LONGITUDE'])\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "\n",
    "# Create markers for each stop and color them based on clusters\n",
    "for idx, row in df_cluster_7.iterrows():\n",
    "    # Retrieve Cluster id and set it as a integer\n",
    "    cluster = int(row['CLUSTER'])\n",
    "\n",
    "    cluster_color = cluster_colors.get(cluster, 'gray')  # Default to gray if cluster color is not defined\n",
    "    #print(cluster_color)\n",
    "\n",
    "    # cluster_color = cluster_\n",
    "    # colors.get(cluster, 'gray')  # Default to gray if cluster color is not defined\n",
    "    # Add a label to the marker with the name of the stop contained in the column 'DESCRIZIONE'\n",
    "    # Retrieve the name of the stop from the column 'DESCRIZIONE' of the dataframe df matching the FERMATA\n",
    "    location = df_cluster_7[df_cluster_7['FERMATA'] == row['FERMATA']]['DESCRIZIONE'].values[0]\n",
    "    # Retrieve FERMATA id and set it as a integer\n",
    "    fermata = int(row['FERMATA'])\n",
    "    # Retrieve the total number of validations for the stop\n",
    "    tot_validations = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'n_V'].iloc[0]\n",
    "    # Retrieve the total number of stops for the stop\n",
    "    tot_stops = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'n_F'].iloc[0]\n",
    "    # Retrieve Cluster id and set it as a integer\n",
    "    cluster = int(row['CLUSTER'])\n",
    "    mean_loc = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_LOC'].iloc[0]\n",
    "    mean_stud = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_STUD'].iloc[0]\n",
    "    mean_tur = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_TUR'].iloc[0]\n",
    "    # Create a string for popup message containing the name of the stop, FERMATA and the cluster in BOLD\n",
    "    #popup_message = f\"<b><i>{location}</i></b><br><br><u>Fermata:</u> {fermata}<br><u>Cluster:</u> {cluster}<br><u>Mean and sd:</u> {mean_0}<br>\"\n",
    "    popup=folium.Popup(f\"<b><i>{location}</i></b><br><br><u>Fermata:</u> {fermata}<br><u>Cluster:</u> {cluster}<br><u>Tot stops:</u> {tot_stops}<br><u>Tot validations:</u> {tot_validations}<br><u>Mean and sd:</u> <i>Mean LOC:</i> {mean_loc}<br><i>Mean STUD:</i> {mean_stud}<br><i>Mean TUR:</i> {mean_tur}<br>\", max_width=300)\n",
    "\n",
    "    if fermata < 0:\n",
    "        folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup, \n",
    "                      icon=folium.Icon(color='white', icon_color=cluster_color, icon='bus', prefix='fa')).add_to(m)\n",
    "    else:\n",
    "        folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup, \n",
    "                      icon=folium.Icon(color='white', icon_color=cluster_color, icon='ship', prefix='fa')).add_to(m)\n",
    "        \n",
    "# Add layer control and show map\n",
    "#m.add_child(folium.LayerControl(collapsed=False))\n",
    "m\n",
    "\n",
    "# export map\n",
    "m.save('map_carnevale.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_compl[dataset_compl['FERMATA']==5090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell me the coordinates of 5056\n",
    "dataset_compl[dataset_compl['FERMATA']==5063]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def find_stops_with_same_clusters(json_data_list):\n",
    "#     all_stops = {}\n",
    "    \n",
    "#     # Iterate through each JSON data\n",
    "#     for json_data in json_data_list:\n",
    "#         for key, data in json_data.items():\n",
    "#             clusters = data['Clusters']\n",
    "\n",
    "#             for cluster_id, cluster_info in clusters.items():\n",
    "#                 stops = cluster_info['Stops']\n",
    "#                 for stop in stops:\n",
    "#                     stop_id = stop['FERMATA']\n",
    "#                     if stop_id not in all_stops:\n",
    "#                         all_stops[stop_id] = set()\n",
    "#                     all_stops[stop_id].add(cluster_id)\n",
    "\n",
    "#     result = {}\n",
    "#     for stop_id, clusters_set in all_stops.items():\n",
    "#         if len(clusters_set) == len(json_data_list):\n",
    "#             cluster_ids = list(clusters_set)\n",
    "#             if len(set(cluster_ids)) == 1:\n",
    "#                 result[stop_id] = cluster_ids[0]\n",
    "\n",
    "#     return result\n",
    "\n",
    "# # Load the JSON data from your files\n",
    "# json_files = ['/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/Documenti/risultati clustering/Nuovo_STUD-LOC-TUR/carnevale_entire/result_carnival_entire.json',\n",
    "#                '/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/Documenti/risultati clustering/Nuovo_STUD-LOC-TUR/easter_entire/result_easter_entire.json',\n",
    "#                  '/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/Documenti/risultati clustering/Nuovo_STUD-LOC-TUR/finePrimavera/result_finePrimavera.json']  # Replace with your file paths\n",
    "\n",
    "# json_data_list = []\n",
    "\n",
    "# for file_name in json_files:\n",
    "#     with open(file_name, 'r') as json_file:\n",
    "#         json_data = json.load(json_file)\n",
    "#         json_data_list.append(json_data)\n",
    "\n",
    "# # Call the function to find stops with the same clusters among the three files\n",
    "# stops_with_same_clusters = find_stops_with_same_clusters(json_data_list)\n",
    "\n",
    "# # Print the result\n",
    "# for stop_id, cluster_id in stops_with_same_clusters.items():\n",
    "#     print(f\"Stop ID: {stop_id}, Cluster ID: {cluster_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def find_stops_with_shared_clusters(json_data_list, threshold=1):\n",
    "#     all_stops = {}\n",
    "#     cluster_stop_counts = {}\n",
    "    \n",
    "#     # Iterate through each JSON data\n",
    "#     for json_data in json_data_list:\n",
    "#         for key, data in json_data.items():\n",
    "#             clusters = data['Clusters']\n",
    "\n",
    "#             for cluster_id, cluster_info in clusters.items():\n",
    "#                 stops = cluster_info['Stops']\n",
    "#                 for stop in stops:\n",
    "#                     stop_id = stop['FERMATA']\n",
    "#                     if stop_id not in all_stops:\n",
    "#                         all_stops[stop_id] = set()\n",
    "#                     all_stops[stop_id].add(cluster_id)\n",
    "\n",
    "#                     # Count the stops in each cluster\n",
    "#                     if cluster_id not in cluster_stop_counts:\n",
    "#                         cluster_stop_counts[cluster_id] = set()\n",
    "#                     cluster_stop_counts[cluster_id].add(stop_id)\n",
    "\n",
    "#     result = {}\n",
    "#     for stop_id, cluster_ids in all_stops.items():\n",
    "#         shared_clusters = []\n",
    "#         for cluster_id in cluster_ids:\n",
    "#             cluster_stop_count = len(cluster_stop_counts[cluster_id])\n",
    "#             if cluster_stop_count / len(json_data_list) >= threshold:\n",
    "#                 shared_clusters.append(cluster_id)\n",
    "        \n",
    "#         if len(shared_clusters) == len(cluster_ids):\n",
    "#             result[stop_id] = shared_clusters\n",
    "\n",
    "#     return result\n",
    "\n",
    "# # Load the JSON data from your files\n",
    "# json_files = ['/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/Documenti/risultati clustering/Nuovo_STUD-LOC-TUR/carnevale_entire/result_carnival_entire.json',\n",
    "#                '/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/Documenti/risultati clustering/Nuovo_STUD-LOC-TUR/easter_entire/result_easter_entire.json',\n",
    "#                  '/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/Documenti/risultati clustering/Nuovo_STUD-LOC-TUR/finePrimavera/result_finePrimavera.json']  # Replace with your file paths\n",
    "\n",
    "# json_data_list = []\n",
    "\n",
    "# for file_name in json_files:\n",
    "#     with open(file_name, 'r') as json_file:\n",
    "#         json_data = json.load(json_file)\n",
    "#         json_data_list.append(json_data)\n",
    "\n",
    "# # Call the function to find stops with at least 75% shared clusters among the three files\n",
    "# stops_with_shared_clusters = find_stops_with_shared_clusters(json_data_list, threshold=1)\n",
    "\n",
    "# # Print the result\n",
    "# for stop_id, shared_clusters in stops_with_shared_clusters.items():\n",
    "#     print(f\"Stop ID: {stop_id}, Shared Clusters: {shared_clusters}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
