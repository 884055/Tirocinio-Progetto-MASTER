{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import folium\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, 'file/carnevale.ipynb')\n",
    "from functions import haversine_distance,euclidean_distance,haversine_distance_normalized,euclidean_distance_normalized,custom_distance, find_csv_files, choose_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = '/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/'\n",
    "dr_data = '/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/data/processed/csvPuliti/'\n",
    "#stops = pd.read_csv(dr+'gtfs/'+'stops.txt', sep=',')\n",
    "#stops.rename(columns={\"stop_id\": \"FERMATA\",\"stop_name\": \"NOME_FERMATA\", \"stop_lat\": \"LATITUDE\", \"stop_lon\": \"LONGITUDE\"}, inplace=True)\n",
    "#cols = ['FERMATA','NOME_FERMATA','LATITUDE','LONGITUDE']\n",
    "#stops = stops[cols]\n",
    "#stops.to_csv(dr+'gtfs/'+ 'stops.csv',index=False)\n",
    "\n",
    "#stops = pd.read_csv(dr+'gtfs/'+ 'stops.csv')\n",
    "#print(f'stops.shape: {stops.shape}')\n",
    "#stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_csv_files(folder_path: str) -> list:\n",
    "    \"\"\"\n",
    "        This function returns a list of all the csv files in the specified folder.\n",
    "        :param folder_path: the path of the folder\n",
    "        :return: a list of all the csv files in the specified folder\n",
    "    \"\"\"\n",
    "\n",
    "    csv_files = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # File extension is .txt or .csv\n",
    "            if file.endswith(\".csv\"):\n",
    "                csv_files.append(os.path.join(root, file))\n",
    "    # Sort the list of txt files in alphabetical order\n",
    "    csv_files.sort()\n",
    "\n",
    "    return csv_files\n",
    "\n",
    "def choose_dataset(txt_files: list) -> str:\n",
    "    \"\"\"\n",
    "        This function returns the path of the txt file chosen by the user.\n",
    "        :param txt_files: the list of txt files\n",
    "        :return: the path of the txt file chosen by the user\n",
    "    \"\"\"\n",
    "    if not txt_files:\n",
    "        print(\"No TXT file found.\")\n",
    "        return \"None\"\n",
    "    if len(txt_files) == 1:\n",
    "        print(\"The following file was found:\")\n",
    "    else:\n",
    "        print(\"The following files were found:\")\n",
    "    for i, file_path in enumerate(txt_files):\n",
    "        print(f\"{i+1}. {file_path}\")\n",
    "    while True:\n",
    "        choice = input(\"Enter the number corresponding to the dataset you wish to use (0 to exit): \")\n",
    "        if not choice.isdigit():\n",
    "            print(\"Enter a valid number.\")\n",
    "            continue\n",
    "        choice = int(choice)\n",
    "        if choice == 0:\n",
    "            return \"None\"\n",
    "        if choice < 1 or choice > len(txt_files):\n",
    "            print(\"Enter a valid number.\")\n",
    "            continue\n",
    "        return txt_files[choice - 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset that has also the geo coordinates\n",
    "# Find all txt files in the data folder\n",
    "csv_file = find_csv_files('/Users/matteograzioso/Desktop/Università/Tirocinio/Tirocinio-Progetto-MASTER/data/processed/csvPuliti/')\n",
    "\n",
    "print(\"Select a dataset with geo coordinates from the list:\")\n",
    "\n",
    "# Choose a dataset from the list of txt files\n",
    "selected_dataset = choose_dataset(csv_file)\n",
    "\n",
    "if selected_dataset:\n",
    "    print(f\"You selected the dataset {selected_dataset}\")\n",
    "else:\n",
    "    print(\"No dataset selected.\")\n",
    "\n",
    "path  = selected_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_notebook = 'file/csvPuliti'\n",
    "# dataset_compl = pd.read_csv(dr_data+'dataset_cleaned_tempesportazionePasqua23_part1.csv')\n",
    "\n",
    "dataset_compl = pd.read_csv(path, header=0, sep=',')\n",
    "\n",
    "\n",
    "print(f'dataset_compl.shape: {dataset_compl.shape}')\n",
    "dataset_compl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_compl.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fermate = dataset_compl['FERMATA'].nunique()\n",
    "print(f'Numero di fermate distinte: {num_fermate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_compl_isola_fermate = dataset_compl.copy()\n",
    "\n",
    "#dataset_compl_isola_fermate = dataset_compl_isola_fermate[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_compl['DATA_VALIDAZIONE'] = pd.to_datetime(dataset_compl['DATA_VALIDAZIONE'])\n",
    "dataset_compl['DATA'] = pd.to_datetime(dataset_compl['DATA'])\n",
    "#dataset_compl.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the interval of dates for which we have data\n",
    "print(f'Date range: {dataset_compl.DATA_VALIDAZIONE.min()} to {dataset_compl.DATA_VALIDAZIONE.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_clust = dataset_compl.copy()\n",
    "#dataset_clust.drop(columns=['DATA', 'ORA','DESCRIZIONE_TITOLO'],inplace=True)\n",
    "dataset_clust.drop(columns=['DESCRIZIONE_TITOLO'],inplace=True)\n",
    "dataset_clust.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_clust.TICKET_CODE.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 5-STUD, 6-STUD to STUD in the dataframe\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].replace(['5-STUD', '6-STUD'], 'STUD')\n",
    "# Change 5-WKRS, 6-WKRS to WKRS in the dataframe\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].replace(['5-WKRS', '6-WKRS'], 'WKRS')\n",
    "# Change 5-RET, 6-RET to RET in the dataframe\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].replace(['5-RET', '6-RET'], 'RET')\n",
    "# Change 5, 6 to LOC (that means locals) in the dataframe\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].replace(['5', '6'], 'LOC')\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].replace(['5', '6'], 'LOC')\n",
    "\n",
    "\n",
    "\n",
    "# Print the unique ticket codes\n",
    "# Print information about the changes made\n",
    "print('The ticket codes 5-STUD and 6-STUD have been changed to STUD')\n",
    "print('The ticket codes 5-WKRS and 6-WKRS have been changed to WKRS')\n",
    "print('The ticket codes 5-RET and 6-RET have been changed to RET')\n",
    "print('The ticket codes 5 and 6 have been changed to LOC')\n",
    "\n",
    "# Convert all the ticket codes to string\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].astype(str)\n",
    "\n",
    "ticket_codes = dataset_clust['TICKET_CODE'].unique()\n",
    "# Sort the ticket codes\n",
    "ticket_codes.sort()\n",
    "\n",
    "print('The considered ticket codes are: ', ticket_codes)\n",
    "\n",
    "# Convert all the ticket codes to string\n",
    "dataset_clust['TICKET_CODE'] = dataset_clust['TICKET_CODE'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fermate = dataset_clust['FERMATA'].nunique()\n",
    "print(f'Numero di fermate distinte: {num_fermate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering in base ai ticket codes e rispetto alla vicinanza geografica delle celle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of stops\n",
    "df_stop = dataset_clust.copy()\n",
    "#df_stop = df_stop[['LATITUDE', 'LONGITUDE', 'CLUSTER','FERMATA']]\n",
    "print(f'df_stop.shape: {df_stop.shape}')\n",
    "df_stop.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_count = df_stop.groupby(['LATITUDE', 'LONGITUDE', 'TICKET_CODE', 'FERMATA', 'DESCRIZIONE']).size().reset_index(name='COUNT')\n",
    "\n",
    "print(f'df_stop_count.shape: {df_stop_count.shape}')\n",
    "df_stop_count.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the column COUNT\n",
    "df_stop_count['COUNT'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stop_count.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for the ticket codes but maintaining the information about the stops\n",
    "df_stop_count = df_stop_count.pivot_table(index=['LATITUDE', 'LONGITUDE', 'FERMATA', 'DESCRIZIONE'], columns='TICKET_CODE', values='COUNT', fill_value=0)\n",
    "df_stop_count.reset_index(inplace=True)\n",
    "\n",
    "# For each stop (LATITUDE, LONGITUDE), change the counter of each ticket code as a percentage of the total number of tickets\n",
    "for row in range(len(df_stop_count)):\n",
    "    total = df_stop_count.iloc[row, 4:].sum()\n",
    "    for col in range(4, len(df_stop_count.columns)):\n",
    "        df_stop_count.iloc[row, col] = df_stop_count.iloc[row, col] / total * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_stop_count.at[0, '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_locations = len(df_stop_count)\n",
    "print(f'num_locations: {num_locations}')\n",
    "haversine_matrix = np.zeros((num_locations, num_locations))\n",
    "euclidean_distances = np.zeros((num_locations, num_locations))\n",
    "\n",
    "for i in range(num_locations):\n",
    "    for j in range(i + 1, num_locations):\n",
    "        coord1 = (df_stop_count.at[i, 'LATITUDE'], df_stop_count.at[i, 'LONGITUDE'])\n",
    "        coord2 = (df_stop_count.at[j, 'LATITUDE'], df_stop_count.at[j, 'LONGITUDE'])\n",
    "        distance = haversine_distance(coord1, coord2)\n",
    "        haversine_matrix[i][j] = distance\n",
    "        haversine_matrix[j][i] = distance  # Since distance is symmetric\n",
    "        \n",
    "        stops1 = [df_stop_count.at[i, '1'], df_stop_count.at[i, '2'], df_stop_count.at[i, '3'],\n",
    "                    df_stop_count.at[i, '4'], df_stop_count.at[i, '7'], df_stop_count.at[i, 'LOC']]\n",
    "        stops2 = [df_stop_count.at[j, '1'], df_stop_count.at[j, '2'], df_stop_count.at[j, '3'],\n",
    "                    df_stop_count.at[j, '4'], df_stop_count.at[j, '7'], df_stop_count.at[j, 'LOC']]\n",
    "        \n",
    "    \n",
    "        stops1 = np.array(stops1)\n",
    "        stops2 = np.array(stops2)\n",
    "\n",
    "        euclidean_dist = euclidean_distance(stops1, stops2)\n",
    "        euclidean_distances[i][j] = euclidean_dist\n",
    "        euclidean_distances[j][i] = euclidean_dist  # Since distance is symmetric\n",
    "        \n",
    "\n",
    "# Create a DataFrame to store the haversine distances\n",
    "haversine_df = pd.DataFrame(haversine_matrix, index=df_stop_count.index, columns=df_stop_count.index)\n",
    "\n",
    "# Find the minimum and maximum haversine distances\n",
    "min_distance = haversine_df.values.min()\n",
    "max_distance = haversine_df.values.max()\n",
    "\n",
    "# Print the haversine distance DataFrame and the minimum and maximum distances\n",
    "#print(\"Haversine Distance DataFrame:\")\n",
    "#print(haversine_df)\n",
    "\n",
    "print(\"Minimum Haversine Distance:\", min_distance, \"km\")\n",
    "print(\"Maximum Haversine Distance:\", max_distance, \"km\")\n",
    "\n",
    "\n",
    "# Create a DataFrame to store the haversine distances\n",
    "euclidean_df = pd.DataFrame(euclidean_distances, index=df_stop_count.index, columns=df_stop_count.index)\n",
    "\n",
    "# Find the minimum and maximum Euclidean distances\n",
    "min_euclidean_distance = euclidean_distances.min()\n",
    "max_euclidean_distance = euclidean_distances.max()\n",
    "avg_euclidean_distance = euclidean_distances.mean()\n",
    "\n",
    "# Print the haversine distance DataFrame and the minimum and maximum distances\n",
    "#print(\"Haversine Distance DataFrame:\")\n",
    "#print(haversine_df)\n",
    "\n",
    "# Now you have the normalized Euclidean distances in the range [0, 1]\n",
    "print(\"Minimum Euclidean Distance:\", min_euclidean_distance.min())\n",
    "print(\"Maximum Euclidean Distance:\", max_euclidean_distance.max())\n",
    "print(\"Average Euclidean Distance:\", avg_euclidean_distance/max_euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min(min_euclidean_distance), max(max_euclidean_distance)\n",
    "max_euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From df_stop_count remove descrizione\n",
    "df_stop_count_data = df_stop_count.copy()\n",
    "df_stop_count_data = df_stop_count_data.drop(columns=['FERMATA','DESCRIZIONE'])\n",
    "\n",
    "data = df_stop_count_data.values\n",
    "print(data.shape)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop 1: 5102.0, stop2: 5103.0, combined_distance: 0.0396467042622267\n",
    "#stop 1: 5102.0, stop2: -5.0, combined_distance: 0.22699456069457805\n",
    "#stop 1: 5102.0, stop2: -2.0, combined_distance: 0.34764672602143193\n",
    "\n",
    "#stop 1: 5050.0, stop2: 5049.0, combined_distance: 0.0017922304598963794 --> stesso cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Custom distance function parameters\n",
    "coord_weight = 0.4\n",
    "similarity_weight = 1-coord_weight\n",
    "print(f'coord_weight: {coord_weight}, similarity_weight: {similarity_weight}')\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "# Calculate linkage matrix using custom distance function\n",
    "# The custom distance function is a weighted average of the haversine distance between coordinates and \n",
    "# the similarity between the stop counts\n",
    "linkage_matrix = linkage(data, method='complete', \n",
    "                         metric=lambda x, y: custom_distance(x, y, coord_weight, similarity_weight,min_distance,\n",
    "                                                             max_distance,min_euclidean_distance,\n",
    "                                                             max_euclidean_distance))\n",
    "\n",
    "\n",
    "\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 20))\n",
    "plt.title('Hierarchical Clustering Dendrogram', fontsize=20)\n",
    "plt.xlabel('cluster', fontsize=20)\n",
    "plt.ylabel('distance', fontsize=20)\n",
    "# Create a dendrogram for visualization purposes \n",
    "dendrogram(linkage_matrix, #truncate_mode='lastp', #p=12, \n",
    "           #orientation=\"right\",\n",
    "           leaf_rotation=90., leaf_font_size=12.)#, show_contracted=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_stop_count[(df_stop_count['FERMATA']==5103) | (df_stop_count['FERMATA']==-2)]\n",
    "#stop 1: 5103.0, stop2: -2.0, combined_distance: 36.09851949056943"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5097 torcello\n",
    "# 5072 verso chioggia \n",
    "# -2 piazzale roma\n",
    "\n",
    "#dataset_time_slots[dataset_time_slots['FERMATA'] == -5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine clusters based on a desired threshold or number of clusters\n",
    "threshold = 0.15\n",
    "clusters = fcluster(linkage_matrix, t=threshold, criterion='distance', depth=2, R=None, monocrit=None)\n",
    "# print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_hd = min(haversine_distance_arr)\n",
    "#max_hd = max(haversine_distance_arr)\n",
    "#avg_hd = np.average(haversine_distance_arr)\n",
    "\n",
    "#min_ed = min(eu_similarity_arr)\n",
    "#max_ed = max(eu_similarity_arr)\n",
    "#avg_ed = np.average(eu_similarity_arr)\n",
    "\n",
    "#print(f'min_hd: {min_hd}, max_hd: {max_hd}, avg_hd: {avg_hd}')\n",
    "#print(f'min_ed: {min_ed}, max_ed: {max_ed}, avg_ed: {avg_ed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(0.7*0.16)\n",
    "#print(0.3*0.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding cluster labels to the stop count dataframe\n",
    "df_stop_count['CLUSTER'] = clusters\n",
    "print(df_stop_count.shape)\n",
    "df_stop_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_stop_count[df_stop_count['CLUSTER']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cl = ['FERMATA','CLUSTER']\n",
    "df_cl = df_stop_count.copy()\n",
    "df_cl = df_cl[cols_cl]\n",
    "\n",
    "dataset_compl = dataset_compl.merge(df_cl, on=['FERMATA'])\n",
    "\n",
    "df_sum_validations = dataset_compl.copy()\n",
    "df_sum_validations = dataset_compl.groupby(['CLUSTER']).size().reset_index()\n",
    "df_sum_validations.rename(columns={0: 'n_V'},inplace=True)\n",
    "df_sum_validations.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'DESCRIZIONE' not in df_stop_count.columns :\n",
    "    df_stop_count = df_stop_count.merge(fermate, on=['LATITUDE','LONGITUDE','FERMATA'])\n",
    "#df_stop_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hex codes of tab_20 palette in the format 1:hex_code dict\n",
    "cluster_colors = dict(zip(range(1,51), sns.color_palette(\"tab20\", 50).as_hex()))\n",
    "#cluster_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countplot of cluster distribution \n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x='CLUSTER', data=df_stop_count)#, palette=cluster_colors)\n",
    "plt.yticks(np.arange(0, max(df_stop_count['CLUSTER'].value_counts()) + 5, 2))\n",
    "plt.title('Cluster Distribution: ' + 'date range: ' + str(df_stop['DATA'].min().date()) + ' to ' \n",
    "          + str(df_stop['DATA'].max().date()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_stop_count.copy()\n",
    "cols_grouped = ['CLUSTER', '1', '2', '3', '4', '7', 'LOC']\n",
    "grouped = grouped[cols_grouped]\n",
    "grouped.reset_index(inplace=True,drop=True)\n",
    "grouped = grouped.rename_axis(None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mean = grouped.groupby(['CLUSTER']).mean()\n",
    "grouped_mean.reset_index(inplace=True)\n",
    "grouped_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è la standard deviation: se vuoi la varianza togli sqrt\n",
    "grouped_var = grouped.groupby(['CLUSTER']).apply(lambda x: np.sqrt(np.var(x)))\n",
    "grouped_var.drop(columns=['CLUSTER'],inplace = True)\n",
    "grouped_var.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = grouped_mean.merge(grouped_var, on=['CLUSTER'])\n",
    "grouped = grouped.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = grouped.round(decimals = 3)\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of ticket codes: 1, 2, 3, 4, 7, LOC, RET, STUD, WKRS\n",
    "list_tc = ['1', '2', '3', '4', '7', 'LOC']\n",
    "for i in list_tc:\n",
    "    col_mean = str(i) + '_x'\n",
    "    col_var = str(i) + '_y'\n",
    "    col_ts = 'Mean_Var_' + str(i)\n",
    "    grouped[col_ts] = grouped[col_mean].astype(str) + ' ± ' + grouped[col_var].apply(lambda x: f'{x:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See all columns of the dataframe\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grouped.shape)\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_final = ['CLUSTER','Mean_Var_1','Mean_Var_2','Mean_Var_3','Mean_Var_4','Mean_Var_7','Mean_Var_LOC']\n",
    "grouped = grouped[cl_final]\n",
    "grouped = grouped.merge(df_sum_validations, on=['CLUSTER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by CLUSTER and aggregate the 'DESCRIZIONE' column into a list\n",
    "clustered_stops = df_stop_count.groupby('CLUSTER')['DESCRIZIONE'].agg(['count', list]).reset_index()\n",
    "clustered_stops = clustered_stops.merge(grouped, on = ['CLUSTER'])\n",
    "#for i in range(6) : \n",
    "#    clustered_stops[i] = clustered_stops[i].round(decimals = 3)\n",
    "#clustered_stops.sort_values(by=[0,1,2,3,4,5])\n",
    "clustered_stops.rename(columns={'count': 'n_F', 'list': 'STOPS'},inplace=True)\n",
    "clustered_cols = ['CLUSTER', 'n_F', 'n_V', 'Mean_Var_1', 'Mean_Var_2', 'Mean_Var_3', 'Mean_Var_4', 'Mean_Var_7', 'Mean_Var_LOC', 'STOPS']\n",
    "clustered_stops = clustered_stops[clustered_cols]\n",
    "clustered_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #df_stop_count[df_stop_count['CLUSTER']==21]\n",
    "# grouped = df_stop_count.copy()\n",
    "# cols_grouped = ['CLUSTER', '1', '2', '3', '4', '7', 'LOC', 'RET', 'STUD', 'WKRS']\n",
    "# grouped = grouped[cols_grouped]\n",
    "# grouped.reset_index(inplace=True,drop=True)\n",
    "# grouped = grouped.rename_axis(None, axis=1)\n",
    "# #grouped\n",
    "\n",
    "# grouped_mean = grouped.groupby(['CLUSTER']).mean()\n",
    "# grouped_mean.reset_index(inplace=True)\n",
    "# grouped_mean\n",
    "\n",
    "# # è la standard deviation: se vuoi la varianza togli sqrt\n",
    "# grouped_var = grouped.groupby(['CLUSTER']).apply(lambda x: np.sqrt(np.var(x)))\n",
    "# grouped_var.drop(columns=['CLUSTER'],inplace = True)\n",
    "# grouped_var.reset_index(inplace=True)\n",
    "\n",
    "# grouped = grouped_mean.merge(grouped_var, on=['CLUSTER'])\n",
    "# grouped = grouped.fillna(0)\n",
    "# #grouped\n",
    "\n",
    "# grouped = grouped.round(decimals = 3)\n",
    "# grouped\n",
    "# grouped\n",
    "\n",
    "# for i in range(6) :  \n",
    "#     col_mean = str(i) + '_x'\n",
    "#     col_var = str(i) + '_y'\n",
    "#     col_ts = 'Mean_Var_' + str(i)\n",
    "#     grouped[col_ts] = grouped[col_mean].astype(str) + ' ± ' + grouped[col_var].apply(lambda x: f'{x:.2f}')\n",
    "\n",
    "# cl_final = ['CLUSTER','Mean_Var_0','Mean_Var_1','Mean_Var_2','Mean_Var_3','Mean_Var_4','Mean_Var_5']\n",
    "# grouped = grouped[cl_final]\n",
    "\n",
    "# #grouped\n",
    "\n",
    "# # Group by CLUSTER and aggregate the 'DESCRIZIONE' column into a list\n",
    "# clustered_stops = df_stop_count.groupby('CLUSTER')['DESCRIZIONE'].agg(['count', list]).reset_index()\n",
    "# clustered_stops = clustered_stops.merge(grouped, on = ['CLUSTER'])\n",
    "# #for i in range(6) : \n",
    "# #    clustered_stops[i] = clustered_stops[i].round(decimals = 3)\n",
    "# #clustered_stops.sort_values(by=[0,1,2,3,4,5])\n",
    "# clustered_stops.rename(columns={'count': 'NUM', 'list': 'STOPS'},inplace=True)\n",
    "# clustered_cols = ['CLUSTER', 'NUM', 'Mean_Var_0', 'Mean_Var_1', 'Mean_Var_2','Mean_Var_3', 'Mean_Var_4',\n",
    "#                   'Mean_Var_5','STOPS']\n",
    "# clustered_stops = clustered_stops[clustered_cols]\n",
    "# clustered_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a folium map centered around the mean latitude and longitude\n",
    "center_lat = np.mean(df_stop_count['LATITUDE'])\n",
    "center_lon = np.mean(df_stop_count['LONGITUDE'])\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=12)\n",
    "\n",
    "# Create markers for each stop and color them based on clusters\n",
    "for idx, row in df_stop_count.iterrows():\n",
    "    # Retrieve Cluster id and set it as a integer\n",
    "    cluster = int(row['CLUSTER'])\n",
    "\n",
    "    cluster_color = cluster_colors.get(cluster, 'gray')  # Default to gray if cluster color is not defined\n",
    "    #print(cluster_color)\n",
    "\n",
    "    # cluster_color = cluster_\n",
    "    # colors.get(cluster, 'gray')  # Default to gray if cluster color is not defined\n",
    "    # Add a label to the marker with the name of the stop contained in the column 'DESCRIZIONE'\n",
    "    # Retrieve the name of the stop from the column 'DESCRIZIONE' of the dataframe df matching the FERMATA\n",
    "    location = df_stop_count[df_stop_count['FERMATA'] == row['FERMATA']]['DESCRIZIONE'].values[0]\n",
    "    # Retrieve FERMATA id and set it as a integer\n",
    "    fermata = int(row['FERMATA'])\n",
    "    # Retrieve the total number of validations for the stop\n",
    "    tot_validations = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'n_V'].iloc[0]\n",
    "    # Retrieve the total number of stops for the stop\n",
    "    tot_stops = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'n_F'].iloc[0]\n",
    "    # Retrieve Cluster id and set it as a integer\n",
    "    cluster = int(row['CLUSTER'])\n",
    "\n",
    "    # Retrieve the mean and standard deviation of the stop counts for the cluster\n",
    "    mean_1 = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_1'].iloc[0]\n",
    "    mean_2 = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_2'].iloc[0]\n",
    "    mean_3 = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_3'].iloc[0]\n",
    "    mean_4 = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_4'].iloc[0]\n",
    "    mean_7 = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_7'].iloc[0]\n",
    "    mean_loc = clustered_stops.loc[clustered_stops['CLUSTER'] == cluster, 'Mean_Var_LOC'].iloc[0]\n",
    "    # Create a string for popup message containing the name of the stop, FERMATA and the cluster in BOLD\n",
    "    popup = folium.Popup(f\"<b><i>{location}</i></b><br><br><u>Fermata:</u> {fermata}<br><u>Cluster:</u> {cluster}<br><u>Tot stops:</u> {tot_stops}<br><u>Tot validations:</u> {tot_validations}<br><u>Mean and sd:</u> <i>Mean 1 day:</i> {mean_1}, <i>Mean 2 days:</i> {mean_2}, <i>Mean 3 days:</i> {mean_3}, <i>Mean 7 days:</i> {mean_4}, <i>Mean 75 minutes:</i> {mean_7}, <i>Mean Locals:</i> {mean_loc}<br>\", max_width=300, min_width=300)\n",
    "\n",
    "    # Add a marker to the map with the popup message and the color of the cluster\n",
    "    # folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup_message, icon=folium.Icon(color=cluster_color)).add_to(m)\n",
    "    # Add a marker to the map with the popup message and the color of the cluster\n",
    "    # folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup_message, icon=folium.Icon(color=cluster_color)).add_to(m)\n",
    "    if fermata < 0:\n",
    "        folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup, \n",
    "                      icon=folium.Icon(color='white', icon_color=cluster_color, icon='bus', prefix='fa')).add_to(m)\n",
    "    else:\n",
    "        folium.Marker(location=[row['LATITUDE'], row['LONGITUDE']], popup=popup, \n",
    "                      icon=folium.Icon(color='white', icon_color=cluster_color, icon='ship', prefix='fa')).add_to(m)\n",
    "\n",
    "# Add layer control and show map\n",
    "#m.add_child(folium.LayerControl(collapsed=False))\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the number of unique values of the column 'FERMATA'\n",
    "print('Number of unique values of the column FERMATA: {}'.format(df_stop_count['FERMATA'].nunique()))\n",
    "# print the numer of clusters\n",
    "print('Number of clusters: {}'.format(df_stop_count['CLUSTER'].nunique()))\n",
    "# Print the average number of stops per cluster\n",
    "print('Average number of stops per cluster: {}'.format(df_stop_count['CLUSTER'].value_counts().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the map to an HTML file\n",
    "\n",
    "th = str(threshold)\n",
    "th = th.replace('.', '')\n",
    "print(f'threshold: {th}')\n",
    "num_cl = str(df_stop_count.CLUSTER.max())\n",
    "print(f'number of clusters: {num_cl}')\n",
    "\n",
    "file_name = 'estate22'\n",
    "dr_map = 'Documenti/risultati clustering/nuovo_13Sett/' + file_name + '/'\n",
    "#time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "try:\n",
    "    m.save(dr+dr_map + file_name + '_threshold_' + th + '_' + num_cl + 'cluster' +'.html')\n",
    "except:\n",
    "    # Create a new folder in map folder\n",
    "    os.mkdir(dr+dr_map + file_name)\n",
    "    m.save(dr+dr_map + file_name + '_threshold_' + th + '_' + num_cl + 'cluster' +'.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For each Cluster id, print the number of stops in that cluster, the FERMATA id and the name of the stop\n",
    "df_stop_count_copy = df_stop_count.copy()\n",
    "# To df_stop_count add a column with the name of the stop matching the FERMATA\n",
    "#df_stop_count_copy['DESCRIZIONE'] = df_stop_count_copy['FERMATA'].\n",
    "#apply(lambda x: df[df['FERMATA'] == x]['DESCRIZIONE'].values[0])\n",
    "\n",
    "# Consider only the columns ['Cluster', 'FERMATA', 'DESCRIZIONE']\n",
    "df_stop_count_copy = df_stop_count_copy[['CLUSTER', 'FERMATA', 'DESCRIZIONE']]\n",
    "df_stop_count_copy.columns.name = None\n",
    "\n",
    "#df_stop_count_copy.groupby(['CLUSTER'])\n",
    "#df_stop_count_copy = df_stop_count_copy.set_index(['CLUSTER','FERMATA'])\n",
    "df_stop_count_copy\n",
    "\n",
    "\n",
    "print('Number of clusters: ', max(df_stop_count_copy['CLUSTER']))\n",
    "print('\\n')\n",
    "for cluster in range(1, max(df_stop_count_copy['CLUSTER']) + 1):\n",
    "    print(f\"CLUSTER {cluster} contains {len(df_stop_count_copy[df_stop_count_copy['CLUSTER'] == cluster])} stops.\")\n",
    "    print(df_stop_count_copy[df_stop_count_copy['CLUSTER'] == cluster][['FERMATA', 'DESCRIZIONE']])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#controlliamo un cluster \n",
    "df_stop_count[df_stop_count['CLUSTER']==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_compl[dataset_compl['FERMATA']==5090]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
