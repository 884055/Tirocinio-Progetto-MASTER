{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASTER - Notebook 3 - Clustering: KMeans, DBSCAN, OPTICS\n",
    "### Matteo Grazioso 884055"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "import re\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "from geopy.geocoders import Nominatim\n",
    "from pandas import Timestamp\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "# import hdbscan\n",
    "from sklearn.cluster import OPTICS\n",
    "import socket\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "import myfunctions as mf # Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disply all columns and all rows\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a file send and emailwith the text of the file\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    "\n",
    "def send_email(sender_email='', sender_password='', receiver_email='', subject='MASTER', file_att:list=[], file_text:str=''):\n",
    "    if not sender_email:\n",
    "        sender_email = 'matteo.grazioso.scuola@outlook.it'\n",
    "    if not sender_password:\n",
    "        sender_password = '***'\n",
    "    if not receiver_email:\n",
    "        receiver_email = '884055@stud.unive.it'\n",
    "    # Outlook SMTP server and port\n",
    "    smtp_server = 'smtp-mail.outlook.com'\n",
    "    smtp_port = 587\n",
    "\n",
    "    # Create a multipart message and set headers\n",
    "    email_message = MIMEMultipart()\n",
    "    email_message['From'] = sender_email\n",
    "    email_message['To'] = receiver_email\n",
    "    email_message['Subject'] = subject\n",
    "\n",
    "    # The message is the text of the file \n",
    "    with open(file_text, 'r') as f:\n",
    "        message = f.read()\n",
    "\n",
    "    # Concatenate to the message my signature\n",
    "    message += '\\n\\n\\n\\nGrazioso Matteo \\nStudente del Corso di Laurea in Informatica â€“ Data Science'\n",
    "\n",
    "    # Add message body\n",
    "    email_message.attach(MIMEText(message, 'plain'))\n",
    "\n",
    "    if file_att:\n",
    "        for file_path in file_att:\n",
    "            # Open the file in bynary\n",
    "            with open(file_path, 'rb') as attachment:\n",
    "                # Add file as application/octet-stream\n",
    "                # Email client can usually download this automatically as attachment\n",
    "                part = MIMEBase('application', 'octet-stream')\n",
    "                part.set_payload(attachment.read())\n",
    "\n",
    "            # Encode file in ASCII characters to send by email    \n",
    "            encoders.encode_base64(part)\n",
    "\n",
    "            # Add header as key/value pair to attachment part\n",
    "            part.add_header('Content-Disposition', 'attachment', filename=file_path)\n",
    "\n",
    "            # Add attachment to message and convert message to string\n",
    "            email_message.attach(part)\n",
    "\n",
    "    # Create SMTP session and login\n",
    "    with smtplib.SMTP(smtp_server, smtp_port) as server:\n",
    "        server.starttls()\n",
    "        server.login(sender_email, sender_password)\n",
    "\n",
    "        # Send email\n",
    "        server.send_message(email_message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send_email(subject='Completion of Algorithm Execution', file_text='email.txt', file_att=['data/plots/esportazioneCompleta/plot_esportazioneCompleta_clusters.png'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_algorithms (dataset_path: str):\n",
    "   # Each dataset already contains the columns that are needed for the analysis in particular geographical coordinates\n",
    "\n",
    "    # For each dataset, execute the following steps:\n",
    "    # 1. Read the dataset\n",
    "    # 2. Drop the columns that are not needed\n",
    "    # 3. Normalize the data\n",
    "    # 4. Clustering of the data with K-Means algorithm: use silhouette score to find the optimal number of clusters\n",
    "    # 5. Plot the clusters on a map with Folium library and save the map as html file\n",
    "    # 6. Export the map as png file\n",
    "    # 7. Represet the clusters with a plot and save the plot as png file\n",
    "    # 8. Clustering with DBSCAN algorithm: use silhouette score to find the optimal number of clusters\n",
    "    # 9. Plot the clusters on a map with Folium library and save the map as html file\n",
    "    # 10. Export the map as png file\n",
    "    # 11. Represet the clusters with a plot and save the plot as png file\n",
    "    # 12. Clustering with HDBSCAN algorithm: use silhouette score to find the optimal number of clusters\n",
    "    # 13. Plot the clusters on a map with Folium library and save the map as html file\n",
    "    # 14. Export the map as png file\n",
    "    # 15. Represet the clusters with a plot and save the plot as png file\n",
    "    # 16. Clustering with OPTICS algorithm: use silhouette score to find the optimal number of clusters\n",
    "    # 17. Plot the clusters on a map with Folium library and save the map as html file\n",
    "    # 18. Export the map as png file\n",
    "    # 19. Represet the clusters with a plot and save the plot as png file\n",
    "\n",
    "    # Repeat the same steps for each dataset\n",
    "    # All the information must be save in a txt file; when each dataset is processed, send the txt file as email attachment\n",
    "\n",
    "    # Create a txt file to save the information\n",
    "    file = open('data/processed/summary.txt', 'w')\n",
    "    file.write('Summary of the clustering analysis\\n\\n')\n",
    "    file.write('Executed in ' + socket.gethostname() + '\\n\\n')\n",
    "    print('Executed in ' + socket.gethostname() + '\\n\\n')\n",
    "\n",
    "    # # Dataset 1: esportazioneCompleta.csv\n",
    "    #\n",
    "    # # 1. Read the dataset (dataset_path)\n",
    "    df = pd.read_csv(dataset_path, sep=',')\n",
    "    # selected_dataset = data/processed/esportazioneCompleta/df_esportazioneCompleta_GEO.csv -> esportazioneCompleta\n",
    "    # data/processed/esportazionePasqua23/df_divided_part1_GEO.csv -> esportazionePasqua23_part1\n",
    "    selected_dataset = dataset_path.split('/')[2].split('.')[0]\n",
    "    # If the dataset has part1 or part2 in the name, mantain it\n",
    "    if 'part' in dataset_path:\n",
    "        selected_dataset = selected_dataset + '_' + dataset_path.split('/')[3].split('_')[2]\n",
    "\n",
    "    print(selected_dataset)\n",
    "    print(len(df))\n",
    "    df.head()\n",
    "    # take only a part of the dataset\n",
    "    percentage = 0.1\n",
    "    df = df[:int(len(df) * percentage)]\n",
    "    file.write('Dataset: ' + selected_dataset + '\\n')\n",
    "    print('Dataset: ' + selected_dataset + '\\n')\n",
    "    file.write('Considered percentage of the dataset: ' + str(percentage * 100) + '%\\n')\n",
    "    print('Considered percentage of the dataset: ' + str(percentage * 100) + '%\\n')\n",
    "    print('Lenght of the dataset: ' + str(len(df)) + '\\n')\n",
    "    file.write('Lenght of the dataset: ' + str(len(df)) + '\\n\\n')\n",
    "    print(df.head())\n",
    "    file.write(df.head().to_string() + '\\n\\n')\n",
    "    # # 2. Drop the columns that are not needed\n",
    "    # Mantain only the columns that are needed for the analysis: DATA ORA, SERIALE, LATITUDINE, LONGITUDINE\n",
    "    df = df[['DATA', 'ORA', 'SERIALE', 'LATITUDE', 'LONGITUDE']]\n",
    "    df.head()\n",
    "    # # 3. Normalize the data\n",
    "    # Normalize the data: convert the column DATA ORA in datetime format\n",
    "    # If ORA contains also the date, remove the date\n",
    "    df['DATA'] = pd.to_datetime(df['DATA'], format='%Y-%m-%d')\n",
    "    if len(df['ORA'][0]) > 8:\n",
    "        df['ORA'] = df['ORA'].apply(lambda x: x.split(' ')[1])\n",
    "    df['ORA'] = pd.to_datetime(df['ORA'], format='%H:%M:%S')\n",
    "    df.head()\n",
    "\n",
    "    # # 4. Clustering of the data with K-Means algorithm: use silhouette score to find the optimal number of clusters\n",
    "    file.write('K-Means clustering\\n\\tStart time: ' + str(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")) + '\\n')\n",
    "    print('K-Means clustering\\n\\tStart time: ' + str(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")) + '\\n')\n",
    "    X = df[['LATITUDE', 'LONGITUDE']]\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # Find the optimal number of clusters\n",
    "    sil = []\n",
    "    kmax = 10\n",
    "    from sklearn.cluster import KMeans\n",
    "    # X_scaled_sil is the first 10% of the dataset\n",
    "    X_scaled_sil = X_scaled[:int(len(X_scaled) * 0.1)] \n",
    "    for k in range(2, kmax+1):\n",
    "        kmeans = KMeans(n_clusters = k).fit(X_scaled_sil)\n",
    "        labels = kmeans.labels_\n",
    "        sil_new = silhouette_score(X_scaled_sil, labels, metric = 'euclidean')\n",
    "        sil.append(sil_new)\n",
    "        print('Silhouette score for ' + str(k) + ' clusters: ' + str(sil_new))\n",
    "    # Plot the silhouette score\n",
    "    plt.plot(sil)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette score')\n",
    "    plt.title('Silhouette score for K-Means clustering')\n",
    "    plt.savefig('data/processed/' + selected_dataset + '/silhouette_score_KMeans.png')\n",
    "    plt.show()\n",
    "    # Apply K-Means clustering with the optimal number of clusters\n",
    "    optimal_number_of_clusters = np.argmax(sil)\n",
    "    file.write('Optimal number of clusters: ' + str(optimal_number_of_clusters) + '\\n')\n",
    "    print('Optimal number of clusters: ' + str(optimal_number_of_clusters) + '\\n')\n",
    "    kmeans = KMeans(n_clusters = optimal_number_of_clusters).fit(X_scaled)\n",
    "    labels = kmeans.labels_\n",
    "    # Add the column CLUSTER to the dataset\n",
    "    df['CLUSTER'] = labels\n",
    "    df.head()\n",
    "    # Evaluate the clustering performance using silhouette score\n",
    "    silhouette_score(X_scaled, labels, metric = 'euclidean')\n",
    "    # Save the information about the clusters in the txt file\n",
    "    file.write('Number of clusters: ' + str(len(df['CLUSTER'].unique())) + '\\n')\n",
    "    print('Number of clusters: ' + str(len(df['CLUSTER'].unique())) + '\\n')\n",
    "    file.write('Silhouette score: ' + str(silhouette_score(X_scaled, labels, metric = 'euclidean')) + '\\n\\n')\n",
    "    print('Silhouette score: ' + str(silhouette_score(X_scaled, labels, metric = 'euclidean')) + '\\n\\n')\n",
    "    for cluster in np.unique(labels):  \n",
    "        file.write(\"Cluster: \" + str(cluster) + '\\n')\n",
    "        print(\"Cluster: \" + str(cluster))\n",
    "        file.write(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "        print(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "\n",
    "    # # 5. Plot the clusters on a map with Folium library and save the map as html file\n",
    "    # Create a map with Folium library\n",
    "    map = folium.Map(location=[df['LATITUDE'].mean(), df['LONGITUDE'].mean()], zoom_start=5)\n",
    "    # Add the clusters to the map\n",
    "    marker_cluster = MarkerCluster().add_to(map)\n",
    "\n",
    "    visited_stops = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        # Only if the coordinates are not already in the map\n",
    "        if [df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']] not in visited_stops:\n",
    "            folium.Marker([df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']], popup=df.iloc[i]['CLUSTER']).add_to(marker_cluster)\n",
    "            visited_stops.append([df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']])\n",
    "    # Save the map as html file\n",
    "    map.save('data/processed/' + selected_dataset + '/map_KMeans.html')\n",
    "\n",
    "    # # 7. Represet the clusters with a plot and save the plot as png file\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.scatterplot(x='LATITUDE', y='LONGITUDE', hue='CLUSTER', data=df, palette='Set1')\n",
    "    plt.title('Clusters for K-Means clustering')\n",
    "    plt.savefig('data/processed/' + selected_dataset + '/clusters_KMeans.png')\n",
    "    plt.show()\n",
    "    file.write('Finished K-Means clustering\\n\\tTime: ' + str(datetime.now().strftime(\"%H:%M:%S\")) + '\\n\\n')\n",
    "    file.close()\n",
    "    # Send also html file as email attachment\n",
    "    send_email(subject = selected_dataset + ' (' + str(percentage * 100) + '%' + ') ' + 'K-Means clustering completed',\n",
    "               file_text='data/processed/summary.txt', \n",
    "               file_att=['data/processed/' + selected_dataset + '/silhouette_score_KMeans.png', \n",
    "                         'data/processed/' + selected_dataset + '/map_KMeans.html', \n",
    "                         'data/processed/' + selected_dataset + '/clusters_KMeans.png'])\n",
    "\n",
    "    # Empty the txt file\n",
    "    file = open('data/processed/summary.txt', 'w')\n",
    "    # Remove the column CLUSTER\n",
    "    df = df.drop(columns=['CLUSTER'])\n",
    "\n",
    "    # # 8. Clustering with DBSCAN algorithm: use silhouette score to find the optimal number of clusters\n",
    "    file = open('data/processed/summary.txt', 'a')\n",
    "    file.write('Summary of the clustering analysis\\n\\n')\n",
    "    file.write('Executed in ' + socket.gethostname() + '\\n\\n')\n",
    "    print('Executed in ' + socket.gethostname() + '\\n\\n')\n",
    "    file.write('Dataset: ' + selected_dataset + '\\n')\n",
    "    print('Dataset: ' + selected_dataset + '\\n')\n",
    "    file.write('Considered percentage of the dataset: ' + str(percentage * 100) + '%\\n')\n",
    "    print('Considered percentage of the dataset: ' + str(percentage * 100) + '%\\n')\n",
    "    print('Lenght of the dataset: ' + str(len(df)) + '\\n')\n",
    "    file.write('Lenght of the dataset: ' + str(len(df)) + '\\n\\n')\n",
    "    print(df.head())\n",
    "    file.write(df.head().to_string() + '\\n\\n')\n",
    "\n",
    "    print('DBSCAN clustering\\n\\tStart time: ' + str(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")) + '\\n')\n",
    "    file.write('Dataset: ' + selected_dataset + '\\n')\n",
    "    print('Dataset: ' + selected_dataset + '\\n')\n",
    "    file.write('DBSCAN clustering\\n\\tStart time: ' + str(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")) + '\\n')\n",
    "    print('DBSCAN clustering\\n\\tStart time: ' + str(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")) + '\\n')\n",
    "    X = df[['LATITUDE', 'LONGITUDE']]\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # Find the optimal number of clusters\n",
    "    sil = []\n",
    "    eps = np.arange(0.1, 1.0, 0.1)\n",
    "    for e in eps:\n",
    "        dbscan = DBSCAN(eps=e, min_samples=5).fit(X_scaled)\n",
    "        labels = dbscan.labels_\n",
    "        sil_new = silhouette_score(X_scaled, labels, metric = 'euclidean')\n",
    "        sil.append(sil_new)\n",
    "        print('Silhouette score for ' + str(e) + ' epsilon: ' + str(sil_new))\n",
    "    # Plot the silhouette score\n",
    "    plt.plot(sil)\n",
    "    plt.xlabel('Epsilon')\n",
    "    plt.ylabel('Silhouette score')\n",
    "    plt.title('Silhouette score for DBSCAN clustering')\n",
    "    plt.savefig('data/processed/' + selected_dataset + '/silhouette_score_DBSCAN.png')\n",
    "    plt.show()\n",
    "    # Apply DBSCAN clustering with the optimal number of clusters\n",
    "    optimal_number_of_clusters = float(eps[np.argmax(sil)])\n",
    "    file.write('Optimal number of clusters: ' + str(optimal_number_of_clusters) + '\\n')\n",
    "    print('Optimal number of clusters: ' + str(optimal_number_of_clusters) + '\\n')\n",
    "    dbscan = DBSCAN(eps=optimal_number_of_clusters, min_samples=5).fit(X_scaled)\n",
    "\n",
    "    labels = dbscan.labels_\n",
    "    # Add the column CLUSTER to the dataset\n",
    "    df['CLUSTER'] = labels\n",
    "    df.head()\n",
    "    # Evaluate the clustering performance using silhouette score\n",
    "    silhouette_score(X_scaled, labels, metric = 'euclidean')\n",
    "    # Save the information about the clusters in the txt file\n",
    "    file.write('Number of clusters: ' + str(len(df['CLUSTER'].unique())) + '\\n')\n",
    "    print('Number of clusters: ' + str(len(df['CLUSTER'].unique())) + '\\n')\n",
    "    file.write('Silhouette score: ' + str(silhouette_score(X_scaled, labels, metric = 'euclidean')) + '\\n\\n')\n",
    "    print('Silhouette score: ' + str(silhouette_score(X_scaled, labels, metric = 'euclidean')) + '\\n\\n')\n",
    "    for cluster in np.unique(labels):\n",
    "        file.write(\"Cluster: \" + str(cluster) + '\\n')\n",
    "        print(\"Cluster: \" + str(cluster))\n",
    "        file.write(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "        print(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "\n",
    "    # # 9. Plot the clusters on a map with Folium library and save the map as html file\n",
    "    # Create a map with Folium library\n",
    "    map = folium.Map(location=[df['LATITUDE'].mean(), df['LONGITUDE'].mean()], zoom_start=5)\n",
    "    # Add the clusters to the map\n",
    "    marker_cluster = MarkerCluster().add_to(map)\n",
    "\n",
    "    visited_stops = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        # Only if the coordinates are not already in the map\n",
    "        if [df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']] not in visited_stops:\n",
    "            folium.Marker([df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']], popup=df.iloc[i]['CLUSTER']).add_to(marker_cluster)\n",
    "            visited_stops.append([df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']])\n",
    "    # Save the map as html file\n",
    "    map.save('data/processed/' + selected_dataset + '/map_DBSCAN.html')\n",
    "\n",
    "    for cluster in np.unique(labels):\n",
    "        file.write(\"Cluster: \" + str(cluster) + '\\n')\n",
    "        print(\"Cluster: \" + str(cluster))\n",
    "        file.write(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "        print(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "\n",
    "    # # 11. Represet the clusters with a plot and save the plot as png file\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.scatterplot(x='LATITUDE', y='LONGITUDE', hue='CLUSTER', data=df, palette='Set1')\n",
    "    plt.title('Clusters for DBSCAN clustering')\n",
    "    plt.savefig('data/processed/' + selected_dataset + '/clusters_DBSCAN.png')\n",
    "    plt.show()\n",
    "    file.write('Finished DBSCAN clustering\\n\\tTime: ' + str(datetime.now().strftime(\"%H:%M:%S\")) + '\\n\\n')\n",
    "    file.close()\n",
    "    # Send also html file as email attachment\n",
    "    send_email(subject= selected_dataset + ' (' + str(percentage * 100) + '%' + ') ' + 'DBSCAN clustering completed',\n",
    "               file_text='data/processed/summary.txt', \n",
    "               file_att=['data/processed/' + selected_dataset + '/silhouette_score_DBSCAN.png', \n",
    "                         'data/processed/' + selected_dataset + '/map_DBSCAN.html', \n",
    "                         'data/processed/' + selected_dataset + '/clusters_DBSCAN.png'])\n",
    "\n",
    "    # Empty the txt file\n",
    "    file = open('data/processed/summary.txt', 'w')\n",
    "    # Remove the column CLUSTER\n",
    "    df = df.drop(columns=['CLUSTER'])\n",
    "\n",
    "    # # # 12. Clustering with HDBSCAN algorithm: use silhouette score to find the optimal number of clusters\n",
    "    # file = open('data/processed/summary.txt', 'a')\n",
    "    # file.write('Summary of the clustering analysis\\n\\n')\n",
    "    # file.write('Dataset: ' + selected_dataset + '\\n')\n",
    "    # file.write('HDBSCAN clustering\\n\\tStart time: ' + str(datetime.now().strftime(\"%H:%M:%S\")) + '\\n')\n",
    "    # X = df[['LATITUDE', 'LONGITUDE']]\n",
    "    # # Standardize the data\n",
    "    # scaler = StandardScaler()\n",
    "    # X_scaled = scaler.fit_transform(X)\n",
    "    # # Find the optimal number of clusters\n",
    "    # sil = []\n",
    "    # min_cluster_size = np.arange(2, 10, 1)\n",
    "    # for m in min_cluster_size:\n",
    "    #     h = hdbscan.HDBSCAN(min_cluster_size=m).fit(X_scaled)\n",
    "    #     labels = h.labels_\n",
    "    #     sil.append(silhouette_score(X_scaled, labels, metric = 'euclidean'))\n",
    "    # # Plot the silhouette score\n",
    "    # plt.plot(sil)\n",
    "    # plt.xlabel('Min cluster size')\n",
    "    # plt.ylabel('Silhouette score')\n",
    "    # plt.title('Silhouette score for HDBSCAN clustering')\n",
    "    # plt.savefig('data/processed/' + selected_dataset + '/silhouette_score_HDBSCAN.png')\n",
    "    # plt.show()\n",
    "    # # Apply HDBSCAN clustering with the optimal number of clusters\n",
    "    # optimal_number_of_clusters = int(min_cluster_size[np.argmax(sil)])\n",
    "    # file.write('Optimal number of clusters: ' + str(optimal_number_of_clusters) + '\\n')\n",
    "    # h = hdbscan.HDBSCAN(min_cluster_size=optimal_number_of_clusters).fit(X_scaled)\n",
    "    # labels = h.labels_\n",
    "    # # Add the column CLUSTER to the dataset\n",
    "    # df['CLUSTER'] = labels\n",
    "    # df.head()\n",
    "    # # Evaluate the clustering performance using silhouette score\n",
    "    # silhouette_score(X_scaled, labels, metric = 'euclidean')\n",
    "    # # Save the information about the clusters in the txt file\n",
    "    # file.write('Number of clusters: ' + str(len(df['CLUSTER'].unique())) + '\\n')\n",
    "    # file.write('Silhouette score: ' + str(silhouette_score(X_scaled, labels, metric = 'euclidean')) + '\\n\\n')\n",
    "    # for cluster in np.unique(labels):\n",
    "    #     file.write(\"Cluster: \" + str(cluster) + '\\n')\n",
    "    #     file.write(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "\n",
    "    # # # 13. Plot the clusters on a map with Folium library and save the map as html file\n",
    "    # # Create a map with Folium library\n",
    "    # map = folium.Map(location=[df['LATITUDE'].mean(), df['LONGITUDE'].mean()], zoom_start=5)\n",
    "    # # Add the clusters to the map\n",
    "    # marker_cluster = MarkerCluster().add_to(map)\n",
    "\n",
    "    # visited_stops = []\n",
    "\n",
    "    # for i in range(len(df)):\n",
    "    #     # Only if the coordinates are not already in the map\n",
    "    #     if [df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']] not in visited_stops:\n",
    "    #         folium.Marker([df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']], popup=df.iloc[i]['CLUSTER']).add_to(marker_cluster)\n",
    "    #         visited_stops.append([df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']])\n",
    "    # # Save the map as html file\n",
    "    # map.save('data/processed/' + selected_dataset + '/map_HDBSCAN.html')\n",
    "\n",
    "    # for cluster in np.unique(labels):\n",
    "    #     file.write(\"Cluster: \" + str(cluster) + '\\n')\n",
    "    #     file.write(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "\n",
    "    # # # 15. Represet the clusters with a plot and save the plot as png file\n",
    "    # # Plot the clusters\n",
    "    # plt.figure(figsize=(10, 10))\n",
    "    # sns.scatterplot(x='LATITUDE', y='LONGITUDE', hue='CLUSTER', data=df, palette='Set1')\n",
    "    # plt.title('Clusters for HDBSCAN clustering')\n",
    "    # plt.savefig('data/processed/' + selected_dataset + '/clusters_HDBSCAN.png')\n",
    "    # plt.show()\n",
    "    # file.write('Finished HDBSCAN clustering\\n\\tTime: ' + str(datetime.now().strftime(\"%H:%M:%S\")) + '\\n\\n')\n",
    "    # file.close()\n",
    "    # # Send also html file as email attachment\n",
    "    # send_email(subject='HDBSCAN clustering completed', file_text='data/processed/summary.txt', file_att=['data/processed/' + selected_dataset + '/silhouette_score_HDBSCAN.png', 'data/processed/' + selected_dataset + '/map_HDBSCAN.html', 'data/processed/' + selected_dataset + '/clusters_HDBSCAN.png'])\n",
    "\n",
    "    # # Empty the txt file\n",
    "    # file = open('data/processed/summary.txt', 'w')\n",
    "    # # Remove the column CLUSTER\n",
    "    # df = df.drop(columns=['CLUSTER'])\n",
    "\n",
    "    # # 16. Clustering with OPTICS algorithm: use silhouette score to find the optimal number of clusters\n",
    "    file = open('data/processed/summary.txt', 'a')\n",
    "    file.write('Summary of the clustering analysis\\n\\n')\n",
    "    print('Summary of the clustering analysis\\n\\n')\n",
    "    file.write('Executed in ' + socket.gethostname() + '\\n\\n')\n",
    "    print('Executed in ' + socket.gethostname() + '\\n\\n')\n",
    "    file.write('Dataset: ' + selected_dataset + '\\n')\n",
    "    print('Dataset: ' + selected_dataset + '\\n')\n",
    "    file.write('Considered percentage of the dataset: ' + str(percentage * 100) + '%\\n')\n",
    "    print('Considered percentage of the dataset: ' + str(percentage * 100) + '%\\n')\n",
    "    print('Lenght of the dataset: ' + str(len(df)) + '\\n')\n",
    "    file.write('Lenght of the dataset: ' + str(len(df)) + '\\n\\n')\n",
    "    print(df.head())\n",
    "    file.write(df.head().to_string() + '\\n\\n')\n",
    "\n",
    "    file.write('OPTICS clustering\\n\\tStart time: ' + str(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")) + '\\n')\n",
    "    print('OPTICS clustering\\n\\tStart time: ' + str(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")) + '\\n')\n",
    "    X = df[['LATITUDE', 'LONGITUDE']]\n",
    "    # Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    # Find the optimal number of clusters\n",
    "    sil = []\n",
    "    min_samples = np.arange(2, 10, 1)\n",
    "    for m in min_samples:\n",
    "        optics = OPTICS(min_samples=m).fit(X_scaled)\n",
    "        labels = optics.labels_\n",
    "        sil_new = silhouette_score(X_scaled, labels, metric = 'euclidean')\n",
    "        sil.append(sil_new)\n",
    "        print('Silhouette score for ' + str(m) + ' min samples: ' + str(sil_new))\n",
    "    # Plot the silhouette score\n",
    "    plt.plot(sil)\n",
    "    plt.xlabel('Min samples')\n",
    "    plt.ylabel('Silhouette score')\n",
    "    plt.title('Silhouette score for OPTICS clustering')\n",
    "    plt.savefig('data/processed/' + selected_dataset + '/silhouette_score_OPTICS.png')\n",
    "    plt.show()\n",
    "    # Apply OPTICS clustering with the optimal number of clusters\n",
    "    optimal_number_of_clusters = int(min_samples[np.argmax(sil)])\n",
    "    file.write('Optimal number of clusters: ' + str(optimal_number_of_clusters) + '\\n')\n",
    "    print('Optimal number of clusters: ' + str(optimal_number_of_clusters) + '\\n')\n",
    "    optics = OPTICS(min_samples=optimal_number_of_clusters).fit(X_scaled)\n",
    "    labels = optics.labels_\n",
    "    # Add the column CLUSTER to the dataset\n",
    "    df['CLUSTER'] = labels\n",
    "    df.head()\n",
    "    # Evaluate the clustering performance using silhouette score\n",
    "    silhouette_score(X_scaled, labels, metric = 'euclidean')\n",
    "    # Save the information about the clusters in the txt file\n",
    "    file.write('Number of clusters: ' + str(len(df['CLUSTER'].unique())) + '\\n')\n",
    "    print('Number of clusters: ' + str(len(df['CLUSTER'].unique())) + '\\n')\n",
    "    file.write('Silhouette score: ' + str(silhouette_score(X_scaled, labels, metric = 'euclidean')) + '\\n\\n')\n",
    "    print('Silhouette score: ' + str(silhouette_score(X_scaled, labels, metric = 'euclidean')) + '\\n\\n')\n",
    "    for cluster in np.unique(labels):\n",
    "        file.write(\"Cluster: \" + str(cluster) + '\\n')\n",
    "        print(\"Cluster: \" + str(cluster))\n",
    "        file.write(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "        print(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "\n",
    "    # # 17. Plot the clusters on a map with Folium library and save the map as html file\n",
    "    # Create a map with Folium library\n",
    "    map = folium.Map(location=[df['LATITUDE'].mean(), df['LONGITUDE'].mean()], zoom_start=5)\n",
    "    # Add the clusters to the map\n",
    "    marker_cluster = MarkerCluster().add_to(map)\n",
    "\n",
    "    visited_stops = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        # Only if the coordinates are not already in the map\n",
    "        if [df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']] not in visited_stops:\n",
    "            folium.Marker([df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']], popup=df.iloc[i]['CLUSTER']).add_to(marker_cluster)\n",
    "            visited_stops.append([df.iloc[i]['LATITUDE'], df.iloc[i]['LONGITUDE']])\n",
    "    # Save the map as html file\n",
    "    map.save('data/processed/' + selected_dataset + '/map_OPTICS.html')\n",
    "\n",
    "    for cluster in np.unique(labels):\n",
    "        file.write(\"Cluster: \" + str(cluster) + '\\n')\n",
    "        print(\"Cluster: \" + str(cluster))\n",
    "        file.write(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "        print(\"Number of Users: \" + str(len(df[df['CLUSTER'] == cluster])) + '\\n\\n')\n",
    "\n",
    "    # # 19. Represet the clusters with a plot and save the plot as png file\n",
    "    # Plot the clusters\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.scatterplot(x='LATITUDE', y='LONGITUDE', hue='CLUSTER', data=df, palette='Set1')\n",
    "    plt.title('Clusters for OPTICS clustering')\n",
    "\n",
    "    plt.savefig('data/processed/' + selected_dataset + '/clusters_OPTICS.png')\n",
    "    plt.show()\n",
    "    file.write('Finished OPTICS clustering\\n\\tTime: ' + str(datetime.now().strftime(\"%H:%M:%S\")) + '\\n\\n')\n",
    "    file.close()\n",
    "    # Send also html file as email attachment\n",
    "    send_email(subject=selected_dataset + ' (' + str(percentage * 100) + '%' + ') ' + 'OPTICS clustering completed',\n",
    "               file_text='data/processed/summary.txt', \n",
    "               file_att=['data/processed/' + selected_dataset + '/silhouette_score_OPTICS.png', \n",
    "                         'data/processed/' + selected_dataset + '/map_OPTICS.html', \n",
    "                         'data/processed/' + selected_dataset + '/clusters_OPTICS.png'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_algorithms('data/processed/esportazioneCompleta/df_esportazioneCompleta_GEO.csv')\n",
    "cluster_algorithms('data/processed/esportazionePasqua23/df_esportazionePasqua23_part1_GEO.csv')\n",
    "cluster_algorithms('data/processed/esportazionePasqua23/df_esportazionePasqua23_part2_GEO.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_algorithms('data/processed/validazioni/df_validazioni_GEO.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
